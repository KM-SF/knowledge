# 概念

+ 每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信。
+ 每个进程都有0-4G的地址空间，其中0-3G用户空间是自己独享的，其他进程没法访问。3G-4G的内核空间，虽然虚拟区域不同，但是所指向的物理区域是相同的。系统能通过内核空间进行进程间通信
+ 常用的进程间通信方式有：
  1. 管道（使用最简单）
  2. 信号（开销最小）
  3. 共享映射区（无血缘关系）
  4. 本地套接字（最稳定）,查看网络章节
  5. FIFO（无血缘关系的管道）

> 进程间通信：![进程间通信](/操作系统/进程/images/进程间通信.png)

# 管道

> #include <unistd.h>
> int pipe(int filedes[2]);
>
> 返回值：成功返回0，失败-1设置errno
>
> + 调用成功返回r/w两个文件描述符，**无需open，但是需要手动close。**
> + 需要一个进程关闭写端，一个进程关闭读端

+ 调用pipe函数时在内核中开辟一块内核缓冲区，是环形队列（称为管道）用于通信，它有一个读端一个写端，然后通过filedes参数传出给用户程序两个文件描述符，**filedes[0]指向管道的读端，filedes[1]指向管道的写端**（很好记，就像0是标准输入1是标准输出一样）。所以管道在用户程序看起来就像一个打开的文件，通过read(filedes[0]);或者write(filedes[1]);
+ **向这个文件读写数据其实是在读写内核缓冲区**。pipe函数调用成功返回0，调用失败返回-1
+ 作用于有血缘关系的进程之间，通过fork来传递，完成数据传递
+ 特质：
  1. 其本质是一个伪文件（实际是内核缓冲区）
  2. 由两个文件描述符引用，一个表示读端，一个表示写端
  3. 规定数据从管道的写端流入管道，从读端流出。半双工通信，一次只能从一个方向通信
+ 局限性：
  1. 数据自己读不能写
  2. 数据一旦被读走，便不再管道中存在，不可反复读取
  3. 由于管道是半双工通信方式，因此数据只能在一个方向流动，只能实现单向通信。
  4. 只能在有公共祖先的进程间使用管道
+ **注意4种特殊情况（假设都是阻塞I/O操作，没有设置O_NONBLOCK标志）：**
  1. 如果所有指向管道写端的文件描述符都关闭了（管道写端的引用计数等于0），而仍然有进程从管道的读端读数据，那么管道中剩余的数据都被读取后，再次read会返回0，就像读到文件末尾一样
  2. 如果有指向管道写端的文件描述符没关闭（管道写端的引用计数大于0），而持有管道写端的进程也没有向管道中写数据，这时有进程从管道读端读数据，那么管道中剩余的数据都被读取后，**再次read会阻塞**，直到管道中有数据可读了才读取数据并返回。
  3. 如果所有指向管道读端的文件描述符都关闭了（管道读端的引用计数等于0），这时有进程向管道的写端write，那么该进程会收到信号SIGPIPE，通常会导致进程异常终止（默认）。
  4. 如果有指向管道读端的文件描述符没关闭（管道读端的引用计数大于0），而持有管道读端的进程也没有从管道中读数据，这时有进程向管道写端写数据，**那么在管道被写满时再次write会阻塞，直到管道中有空位置了才写入数据并返回。**

> 管道步骤：![管道步骤](/操作系统/进程/images/管道步骤.png)

# FIFO有名管道

> 创建一个有名管道，**解决无血缘关系**的进程通信：FIFO
>
> #include <sys/types.h>
> #include <sys/stat.h>
> int mkfifo(const char *pathname, mode_t mode);

+ **当只写打开FIFO管道时，如果没有FIFO没有读端打开，则open写打开会阻塞。**
+  **FIFO内核实现时可以支持双向通信，也是一个缓冲区。（pipe单向通信，因为父子进程共享同一个file结构体）**
+  **FIFO可以一个读端，多个写端；也可以一个写端，多个读端。**

> FIFIO：![FIFO](/操作系统/进程/images/FIFO.png)

# 共享内存

> #include <sys/mman.h>
> void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
>
> + addr参数：NULL，内核会自己在进程地址空间中选择合适的地址建立映射
> + len参数：需要映射的那一部分文件的长度
> + off参数：从文件的什么位置开始映射，必须是页大小的整数倍（在32位体系统结构上通常是4K）
> + prot参数有四种取值：
>   + PROT_EXEC表示映射的这一段可执行，例如映射共享库
>   + PROT_READ表示映射的这一段可读
>   + PROT_WRITE表示映射的这一段可写
>   + PROT_NONE表示映射的这一段不可访问
> + flag参数：有很多种取值，这里只讲两种
>   + MAP_SHARED：
>     + 多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修改，另一个进程也会看到这种变化。
>     + 映射的文件会被修改
>   + MAP_PRIVATE：
>     + 多个进程对同一个文件的映射不是共享的，一个进程对映射的内存做了修改，另一个进程并不会看到这种变化。
>     + 映射的文件不会被修改
> + 返回值：成功则返回映射首地址，如果出错则返回常数**MAP_FAILED**。
>
> int munmap(void *addr, size_t length);
>
> + 当进程终止时，该进程的映射内存会自动解除，也可以调用munmap解除映射。
> + addr：申请的首地址
> + length：申请的总长度
> + 成功返回0，出错返回-1

+ mmap可以把**磁盘文件的一部分直接映射到内存（内核空间）**，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用**指针**来做而不需要read/write函数。
+ mmap是内核借助文件帮我创建了一个映射区，多个进程之间利用该映射区完成数据传递。内核空间多进程共享。
+ 用于进程间通信时，一般设计成结构体，来传输通信的数据
* 进程间通信的文件，应该设计成临时文件
* 当报总线错误时，优先查看共享文件是否有存储空间
+ **注意事项**
  1. 创建映射区的过程中，隐含着一次对映射文件的读操作。所以open的文件一定要有读权限。
  2. 当MAP_SHARED时，要求：映射区的权限<=文件打开的权限（处于对映射区的保护）。
  3. 当MAP_PRIVATE时，则对文件打开的权限有读的权限即可。
  4. 映射区的释放与文件关闭无关，只要映射建立成功，文件可以立即关闭
  5. 特别注意，当映射文件大小为0时，不能创建映射区，所以用于映射的文件必须要有实际大小
  6. munmap：传入的地址一定是mmap返回的地址，杜绝创建其他地址。
  7. 偏移量必须是4K的整数倍
  8. mmap创建映射区出错概率非常高，一定要检查返回值，确保映射区建立成功后再进行操作

> mmap：![mmap](/操作系统/进程/images/mmap.png)

## mmap父子进程通信

+ 父子等有血缘关系的进程之间也可以通过mmap建立的映射区来完成数据通信，
+ 创建映射区的时候对应的标志位参数flags：
  + MAP_PRIVATE：私有映射，父子进程各自独享映射区
  + MAP_SHARED：共享映射区，父子进程共享映射区
+ 父子进程贡献：
  1. 打开的文件描述符对应的文件指针 
  2. mmap建立的映射区

## 匿名映射

+ 系统给我提供了创建匿名映射的方法，无需依赖一个文件即可创建映射区。

+ 需要借助标记位参数flags来指定，MAP_ANONYMOUS（或MAP_ANON）

  ```c
  // length：可以填写自己实际需要的大小
  // fd：传-1
  int *p = mmap(NULL, 4, PROT_READ|PROT_WRITE|, MAP_SHARED|MAP_ANONYMOUS, -1, 0)
  ```

+ MAP_ANONYMOUS（或MAP_ANON）这个两个宏是linux特有的，unix系统没有。unix可以使用

  ```C
  fd = open("/dev/zero", O_RDWR);
  int *p = mmap(NULL, 4, PROT_READ|PROT_WRITE|, MAP_SHARED, fd, 0)
  ```

## mmap无血缘关系进程间通信

+ **实际上mmap是内核借助文件帮我创建了一个映射区，多个进程之间利用该映射区完成数据传递。由于内核空间多进程共享，所以无血缘关系的进程也可以使用mmap完成通信，只要设置标记位flags为：MAP_SHARED，就可以实现通信**
+ 可多读端，多写端

## mmap和read/write区别

page cache。内核会为**每个文件**单独维护一个page cache，用户进程对于文件的大多数读写操作会直接作用到page cache上，内核会选择在适当的时候将page cache中的内容写到磁盘上（当然我们可以手工fsync控制回写），这样可以大大减少磁盘的访问次数，从而提高性能。Page cache是[linux](https://so.csdn.net/so/search?q=linux&spm=1001.2101.3001.7020)内核文件访问过程中很重要的[数据结构](http://lib.csdn.net/base/datastructure)，page cache中会保存用户进程访问过得该文件的内容，这些内容以页为单位保存在内存中，当用户需要访问文件中的某个偏移量上的数据时，内核会以偏移量为索引，找到相应的内存页，如果该页没有读入内存，则需要访问磁盘读取数据。为了提高页得查询速度同时节省page cache数据结构占用的内存，linux内核使用树来保存page cache中的页。

### read/write系统调用会有以下的操作：

1. 访问文件，这涉及到用户态到内核态的转换
2. 读取硬盘文件中的对应数据，内核会采用预读的方式，比如我们需要访问100字节，内核实际会将按照4KB(内存页的大小)存储在page cache中
3. 将read中需要的数据，从page cache中拷贝到用户缓冲区中

### mmap系统调用会有以下的操作：

mmap系统调用是将硬盘文件映射到用内存中，说的底层一些是将page cache中的页直接映射到用户进程地址空间中，从而进程可以直接访问自身地址空间的虚拟地址来访问page cache中的页，这样不会涉及page cache到用户缓冲区之间的拷贝

1. **mmap只需要一次系统调用，后续操作不需要系统调用**
2. **访问的数据不需要在page cache和用户缓冲区之间拷贝**

# mmap和零拷贝

## 基础知识

![](/操作系统/进程/images/内存拷贝.png)

文件缓冲区，socket buffer是内核缓冲区，但是两者位置不同，不是同一个缓冲区。磁盘缓冲区，网卡缓冲区在硬件设备上。
读写文件100M文件，不是分配100M内存，而是分配64k内存循环读写，否则直接分配100M，其他应用就容易被挂起。
send是写内存缓冲区成功,不是写到网络成功,网络可能一直发送不完。所以下次send的时候可能失败,因为buffer满,没发送出去.

## 传统IO

一般情况下，我们需要调用read读取数据，然后处理完数据后调用write发送。那么来看下这个流程数据是如何工作

![](/操作系统/进程/images/传统IO.png)

上图中，上半部分表示用户态和内核态的上下文切换。下半部分表示数据复制操作。下面说说他们的步骤：

1. read 调用导致**用户态到内核态的一次变化**，同时，第一次复制开始：DMA（Direct Memory Access，直接内存存取，即不使用 CPU 拷贝数据到内存，而是 DMA 引擎传输数据到内存，用于解放 CPU） 引擎**从磁盘读取文件，并将数据放入到内核缓冲区。**

2. 发生第二次数据拷贝，即：**将内核缓冲区的数据拷贝到用户缓冲区**，同时，**发生了一次用内核态到用户态的上下文切换。**

3. 发生第三次数据拷贝，我们调用 write 方法，**系统将用户缓冲区的数据拷贝到 Socket 缓冲区**。此时，**又发生了一次用户态到内核态的上下文切换。**

4. 第四次拷贝，**数据异步的从 Socket 缓冲区，使用 DMA 引擎拷贝到网络协议引擎**。这一段，不需要进行上下文切换。

5. write 方法返回，**再次从内核态切换到用户态。**

**总结：传统IO需要发生4次数据拷贝，4次上下文切换**

## mmap

![](/操作系统/进程/images/mmap数据拷贝.png)

mmap 通过内存映射，将文件映射到内核缓冲区，同时，用户空间可以共享内核空间的数据。这样，在进行网络传输时，就可以**减少内核空间到用户空间的拷贝次数**

**总结：MMAP需要发生3次数据拷贝，4次上下文切换**

## sendfile（零拷贝）

###  Linux 2.1 版本

![](/操作系统/进程/images/sendfile数据拷贝1.png)

 [Linux](https://so.csdn.net/so/search?q=Linux&spm=1001.2101.3001.7020) 2.1 版本 提供了 sendFile 函数，其基本原理如下：数据根本不经过用户态，直接从内核缓冲区进入到 Socket Buffer，同时，由于和用户态完全无关，就**减少了一次上下文切换。**

**总结：sendfile正常情况下需要3次数据拷贝，3次上下文切换**

###  Linux 2.4 版本

![](/\操作系统/进程/images/sendfile数据拷贝2.png)

Linux 在 2.4 版本中，做了一些修改，避免了从内核缓冲区拷贝到 Socket buffer 的操作，直接拷贝到协议栈，从而再一次减少了数据拷贝。

现在，从文件进入到网络协议栈，只需 2 次拷贝：第一次使用 DMA 引擎从文件拷贝到内核缓冲区，第二次从内核缓冲区将数据拷贝到网络协议栈；内核缓存区只会拷贝一些 offset 和 length 信息到 SocketBuffer，基本无消耗。

**总结：sendfile正常情况下需要2次数据拷贝，3次上下文切换**

## mmap和sendfile区别

1. mmap 适合小数据量读写，sendFile 适合大文件传输。
2. mmap 需要 4 次上下文切换，3 次数据拷贝；sendFile 需要 3 次上下文切换，最少 2 次数据拷贝。
3. sendFile 可以利用 DMA 方式，减少 CPU 拷贝，mmap 则不能（必须从内核拷贝到 Socket 缓冲区）
4. mmap的本质,就是进程可以访问内核态的页缓存,减少了一次内核态到用户态的拷贝.
5. sendfile的本质,是网络DMA直接读取内核页缓存,减少了一次内核页缓存到socket 缓冲区的拷贝

------

参考：

https://blog.csdn.net/fdsafwagdagadg6576/article/details/107584821