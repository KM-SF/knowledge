# 概念

+ 每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信。
+ 每个进程都有0-4G的地址空间，其中0-3G用户空间是自己独享的，其他进程没法访问。3G-4G的内核空间，虽然虚拟区域不同，但是所指向的物理区域是相同的。系统能通过内核空间进行进程间通信
+ 常用的进程间通信方式有：
  1. 管道（使用最简单）
  2. 信号（开销最小）
  3. 共享映射区（无血缘关系）
  4. 本地套接字（最稳定）,查看网络章节
  5. FIFO（无血缘关系的管道）

> 进程间通信：![进程间通信](/操作系统/进程/images/进程间通信.png)

# 管道

> #include <unistd.h>
> int pipe(int filedes[2]);
>
> 返回值：成功返回0，失败-1设置errno
>
> + 调用成功返回r/w两个文件描述符，**无需open，但是需要手动close。**
> + 需要一个进程关闭写端，一个进程关闭读端

+ 调用pipe函数时在内核中开辟一块内核缓冲区，是环形队列（称为管道）用于通信，它有一个读端一个写端，然后通过filedes参数传出给用户程序两个文件描述符，**filedes[0]指向管道的读端，filedes[1]指向管道的写端**（很好记，就像0是标准输入1是标准输出一样）。所以管道在用户程序看起来就像一个打开的文件，通过read(filedes[0]);或者write(filedes[1]);
+ **向这个文件读写数据其实是在读写内核缓冲区**。pipe函数调用成功返回0，调用失败返回-1
+ 作用于有血缘关系的进程之间，通过fork来传递，完成数据传递
+ 特质：
  1. 其本质是一个伪文件（实际是内核缓冲区）
  2. 由两个文件描述符引用，一个表示读端，一个表示写端
  3. 规定数据从管道的写端流入管道，从读端流出。半双工通信，一次只能从一个方向通信
+ 局限性：
  1. 数据自己读不能写
  2. 数据一旦被读走，便不再管道中存在，不可反复读取
  3. 由于管道是半双工通信方式，因此数据只能在一个方向流动，只能实现单向通信。
  4. 只能在有公共祖先的进程间使用管道
+ **注意4种特殊情况（假设都是阻塞I/O操作，没有设置O_NONBLOCK标志）：**
  1. 如果所有指向管道写端的文件描述符都关闭了（管道写端的引用计数等于0），而仍然有进程从管道的读端读数据，那么管道中剩余的数据都被读取后，再次read会返回0，就像读到文件末尾一样
  2. 如果有指向管道写端的文件描述符没关闭（管道写端的引用计数大于0），而持有管道写端的进程也没有向管道中写数据，这时有进程从管道读端读数据，那么管道中剩余的数据都被读取后，**再次read会阻塞**，直到管道中有数据可读了才读取数据并返回。
  3. 如果所有指向管道读端的文件描述符都关闭了（管道读端的引用计数等于0），这时有进程向管道的写端write，那么该进程会收到信号SIGPIPE，通常会导致进程异常终止（默认）。
  4. 如果有指向管道读端的文件描述符没关闭（管道读端的引用计数大于0），而持有管道读端的进程也没有从管道中读数据，这时有进程向管道写端写数据，**那么在管道被写满时再次write会阻塞，直到管道中有空位置了才写入数据并返回。**

> 管道步骤：![管道步骤](/操作系统/进程/images/管道步骤.png)

# FIFO有名管道

> 创建一个有名管道，**解决无血缘关系**的进程通信：FIFO
>
> #include <sys/types.h>
> #include <sys/stat.h>
> int mkfifo(const char *pathname, mode_t mode);

+ **当只写打开FIFO管道时，如果没有FIFO没有读端打开，则open写打开会阻塞。**
+  **FIFO内核实现时可以支持双向通信，也是一个缓冲区。（pipe单向通信，因为父子进程共享同一个file结构体）**
+  **FIFO可以一个读端，多个写端；也可以一个写端，多个读端。**

> FIFIO：![FIFO](/操作系统/进程/images/FIFO.png)

# 共享内存

> #include <sys/mman.h>
> void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
>
> + addr参数：NULL，内核会自己在进程地址空间中选择合适的地址建立映射
> + len参数：需要映射的那一部分文件的长度
> + off参数：从文件的什么位置开始映射，必须是页大小的整数倍（在32位体系统结构上通常是4K）
> + prot参数有四种取值：
>   + PROT_EXEC表示映射的这一段可执行，例如映射共享库
>   + PROT_READ表示映射的这一段可读
>   + PROT_WRITE表示映射的这一段可写
>   + PROT_NONE表示映射的这一段不可访问
> + flag参数：有很多种取值，这里只讲两种
>   + MAP_SHARED：
>     + 多个进程对同一个文件的映射是共享的，一个进程对映射的内存做了修改，另一个进程也会看到这种变化。
>     + 映射的文件会被修改
>   + MAP_PRIVATE：
>     + 多个进程对同一个文件的映射不是共享的，一个进程对映射的内存做了修改，另一个进程并不会看到这种变化。
>     + 映射的文件不会被修改
> + 返回值：成功则返回映射首地址，如果出错则返回常数**MAP_FAILED**。
>
> int munmap(void *addr, size_t length);
>
> + 当进程终止时，该进程的映射内存会自动解除，也可以调用munmap解除映射。
> + addr：申请的首地址
> + length：申请的总长度
> + 成功返回0，出错返回-1

+ mmap可以把**磁盘文件的一部分直接映射到内存（内核空间）**，这样文件中的位置直接就有对应的内存地址，对文件的读写可以直接用**指针**来做而不需要read/write函数。
+ mmap是内核借助文件帮我创建了一个映射区，多个进程之间利用该映射区完成数据传递。内核空间多进程共享。
+ 用于进程间通信时，一般设计成结构体，来传输通信的数据
* 进程间通信的文件，应该设计成临时文件
* 当报总线错误时，优先查看共享文件是否有存储空间
+ **注意事项**
  1. 创建映射区的过程中，隐含着一次对映射文件的读操作。所以open的文件一定要有读权限。
  2. 当MAP_SHARED时，要求：映射区的权限<=文件打开的权限（处于对映射区的保护）。
  3. 当MAP_PRIVATE时，则对文件打开的权限有读的权限即可。
  4. 映射区的释放与文件关闭无关，只要映射建立成功，文件可以立即关闭
  5. 特别注意，当映射文件大小为0时，不能创建映射区，所以用于映射的文件必须要有实际大小
  6. munmap：传入的地址一定是mmap返回的地址，杜绝创建其他地址。
  7. 偏移量必须是4K的整数倍
  8. mmap创建映射区出错概率非常高，一定要检查返回值，确保映射区建立成功后再进行操作

> mmap：![mmap](/操作系统/进程/images/mmap.png)

## mmap父子进程通信

+ 父子等有血缘关系的进程之间也可以通过mmap建立的映射区来完成数据通信，
+ 创建映射区的时候对应的标志位参数flags：
  + MAP_PRIVATE：私有映射，父子进程各自独享映射区
  + MAP_SHARED：共享映射区，父子进程共享映射区
+ 父子进程贡献：
  1. 打开的文件描述符对应的文件指针 
  2. mmap建立的映射区

## 匿名映射

+ 系统给我提供了创建匿名映射的方法，无需依赖一个文件即可创建映射区。

+ 需要借助标记位参数flags来指定，MAP_ANONYMOUS（或MAP_ANON）

  ```c
  // length：可以填写自己实际需要的大小
  // fd：传-1
  int *p = mmap(NULL, 4, PROT_READ|PROT_WRITE|, MAP_SHARED|MAP_ANONYMOUS, -1, 0)
  ```

+ MAP_ANONYMOUS（或MAP_ANON）这个两个宏是linux特有的，unix系统没有。unix可以使用

  ```C
  fd = open("/dev/zero", O_RDWR);
  int *p = mmap(NULL, 4, PROT_READ|PROT_WRITE|, MAP_SHARED, fd, 0)
  ```

## mmap无血缘关系进程间通信

+ **实际上mmap是内核借助文件帮我创建了一个映射区，多个进程之间利用该映射区完成数据传递。由于内核空间多进程共享，所以无血缘关系的进程也可以使用mmap完成通信，只要设置标记位flags为：MAP_SHARED，就可以实现通信**
+ 可多读端，多写端

## mmap和read/write区别

page cache。内核会为**每个文件**单独维护一个page cache，用户进程对于文件的大多数读写操作会直接作用到page cache上，内核会选择在适当的时候将page cache中的内容写到磁盘上（当然我们可以手工fsync控制回写），这样可以大大减少磁盘的访问次数，从而提高性能。Page cache是[linux](https://so.csdn.net/so/search?q=linux&spm=1001.2101.3001.7020)内核文件访问过程中很重要的[数据结构](http://lib.csdn.net/base/datastructure)，page cache中会保存用户进程访问过得该文件的内容，这些内容以页为单位保存在内存中，当用户需要访问文件中的某个偏移量上的数据时，内核会以偏移量为索引，找到相应的内存页，如果该页没有读入内存，则需要访问磁盘读取数据。为了提高页得查询速度同时节省page cache数据结构占用的内存，linux内核使用树来保存page cache中的页。

### read/write系统调用会有以下的操作：

1. 访问文件，这涉及到用户态到内核态的转换
2. 读取硬盘文件中的对应数据，内核会采用预读的方式，比如我们需要访问100字节，内核实际会将按照4KB(内存页的大小)存储在page cache中
3. 将read中需要的数据，从page cache中拷贝到用户缓冲区中

### mmap系统调用会有以下的操作：

mmap系统调用是将硬盘文件映射到用内存中，说的底层一些是将page cache中的页直接映射到用户进程地址空间中，从而进程可以直接访问自身地址空间的虚拟地址来访问page cache中的页，这样不会涉及page cache到用户缓冲区之间的拷贝

1. **mmap只需要一次系统调用，后续操作不需要系统调用**
2. **访问的数据不需要在page cache和用户缓冲区之间拷贝**