### 分层模型

OSI七层模型：物，数，网，传，会，表，应

TCP/IP四层模型：网（数据链路层/网络接口层），网，传，应

OSI七层模型中：应用层 + 表示层 + 会话层 = TPC/IP四层协议：应用层

OSI七层模型中：数据链路层 + 物理层 = TPC/IP四层协议：数据链路层

| OIS七层模型 | TCP/IP四层模型 | 协议                       |
| :---------- | :------------- | -------------------------- |
| 应用层      | 应用层         | http,ftp,nfs,ssh,telnet... |
| 表示层      |                |                            |
| 会话层      |                |                            |
| 传输层      | 传输层         | TCP,UDP                    |
| 网络层      | 网络层         | IP,ICMP,IGMP               |
| 数据链路层  | 数据链路层     | 以太网帧协议，ARP          |
| 物理层      |                |                            |

### TCP如何保证可靠性

答：最大报文长度（mss），ACK到达确认，超时重传，seq序列号发送，数据校验，滑动窗口/拥塞控制

1. 序号/应答机制：数据传输的顺序行
2. 超时重传机制：发送数据后，启动计时器，在超时时间内没有收到ack确认序号，会重发数据包
3. 数据头部校验：TCP保证它的头部和数据的校验和，如果数据包有差错，就丢弃数据不进行ACK
4. 滑动窗口：告诉对方，我这里的缓冲区剩余大小是多少。防止对方发送的数据量大于我缓冲区剩下的大小
5. 拥塞控制：在一方面保证了TCP数据传送的可靠性。然而如果网络非常拥堵，此时再发送数据就会加重网络负担，那么发送的数据段很可能超过了最大生存时间也没有到达接收方，就会产生丢包问题。为此TCP引入慢启动机制，先发出少量数据，就像探路一样，先摸清当前的网络拥堵状态后，再决定按照多大的速度传送数据。
6. 最大报文长度：在建立TCP的时候，会约定数据包的最大长度，超过该长度会进行切分

### 确认/重传机制

问题：确认和重传机制下如果1，2，4，5号的ACK收到了，3号丢失，那么发送端仅重发3号还是全部重发3，4，5？

答： 

​	超时重传：就是发送端发送seq3。

​	快速重传：则是接收端连续发送ACK=3的确认序号，发送端收到了三个 Ack = 3 的确认，知道了 Seq3 还没有收到，就会在定时器过期之前，重传丢失的 Seq3。

### 超时重传时间间隔和次数

指数退避方式

第一次发送数据后，设置的超时时间是1.5s，此后每次重传增加1倍，一直到64秒

一共重传12次，大约9分钟才放弃

### TCP粘包/拆包

+ 现象：因为TCP是流式套接字，是没有界限的一串数据。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区实际情况进行包的划分。所以，在业务层上认为，一个完整的包可能被TCP拆分成多个包进行发送，也可能把多个小包封装成一个大包发送。这就是所谓的粘包和拆包
+ 本质原因：
  + 要发送的数据小于TCP发送缓冲区的大小，多次写入缓冲区的数据一次性发送出去，发生粘包
  + 要发送的数据大于TCP发送缓冲区剩余空间大小，被拆分成多个数据包发送，发生拆包
  + 待发送的数据大于MSS（最大报文长度）,TCP在传输钱进行拆包
+ 解决方案：由于底层TCP无法知道业务层数据传输情况，所以TCP底层无法保证不发生粘包和拆包情况。这个只能业务层的应用协议来控制
  + 消息定长：发送端将每个数据包封装为固定长度的数据块（长度不够补0填充）
  + 设置消息边界：包尾部添加\n\r，如：FTP协议
  + 消息封装：将消息数据封装为struct{int len; void *data}; 指定数据的长度



### 服务器接入抖动如何解决

+ 现象：例如一个服务器正常情况下接入10K个连接，耗时1000ms。突然有那么一次10K个连接耗时5000ms。耗时时间翻倍称为接入抖动
+ 原因：服务器在开启socket监听和连接的时候，底层会有一个半连接队列和一个全连接队列。**全连接队列满了，没有及时取走fd。**accept函数调用不及时，导致fd没有被取出来。
  + 半连接队列：当客户端调用connect函数是，发送了syn请求给服务器。服务器用listen收到该请求，并且将fd加入到一个半连接队列
  + 全连接队列：服务器用listen收到请求后，回一个syn+ack请求。客户端再回一个ack请求，服务器收到ack回复的时候，将半连接队列的fd拷贝到全连接队列。然后调用accept函数将fd从全连接队列中取出来。
  + 类似于生产者和消费者。客户端不断发起接入生产，服务器的队列满了，没被消费
+ 解决办法：
  + 将listen的第二个参数设置大一点，可以延缓出现的概率（不能根治）
  + 用多线程或者多进程的方式，并行调用accept函数。

### ！！！三次握手过程

![3次握手](/网络/images/TCP三次握手.jpg)

1. 客户端向服务端发送连接请求报文（客户端进入**SYN-SEND**状态）。SYN标志位为1，seq序号为**n**。
2. 服务端收到客户端请求（服务端结束**LISTEN**状态）。发送应答报文（服务端进入**SYN-RCVD**状态）。SYN标记位为1，seq序号为**k**。ACK标记位为1，ack确认序号为**n+1**
3. 客户端收到应答报文，再发送一个应答报文给服务端。（客户端和服务端状态进入**ESTABLISHEN**）。ACK标记位为1，ack确认序号为**k+1**，seq序号为n+1。

### ！！！为什么要进行三次握手？而不是2次或者4次

答：执行完步骤1，2后。C给S发送了连接请求，并且S回复了C可以连接，此时C知道自己和S端没问题了。但是S端不知道C端是否没问题。因为需要步骤3，让C告诉S没问题。这也S和C都互相知道对方没问题，连接建立成功

>  两次握手：**防止已失效的请求报文段突然又传送到了服务端而造成连接的误判**
>
> ​	假如客户端发出连接请求A，由于网络原因，服务端并没有收到A，于是客户端又发送了连接请求B，并建立了连接，完成通信，断开连接。这时候，服务端突然又收到了A，于是看作是一次新的连接请求，进行第二次握手，由于不存在第三次握手，所以这时已经建立了TCP连接。但实际上客户端并没有发起连接，所以不会传递数据，那么这条连接就会变成一条死连接。络延迟，延迟的连接请求后面可能会再次发送给服务端，服务端却不知道这个连接是之前的连接，就会与延迟的连接建立新连接
>

> 四次握手：
>
> ​	3次握手已经能够让双端都确认建立了有效的连接，再多一次造成浪费

### ！！！四次挥手过程

四次挥手时序图：

<img src="/网络/images/TCP四次挥手.jpg" alt="四次挥手时序图" style="zoom:150%;" />

过程：

1. 客户端向服务器发送断开连接报文（客户端进入**FIN-WAIT-1**阶段），FIN标记位为1，seq为X（随机值）
2. 服务器收到客户端发来的断开连接报文后，发送确认报文（服务器结束**ESTABLISHEN**，进去**CLOSE-WAIT**阶段），ACK标记位位1，ack确认序号**X+1**，seq序号为Y随机值。客户端收到服务器回的的确认报文后，客户端进入**半关闭状态**（退出**FIN-WAIT-1**阶段，进入**FIN-WAIT-2**阶段）
3. 服务器发送断开连接报文（服务器退出**CLOSE-WAIT**阶段，进入**LAST-ACK**阶段）。FIN标记为为1，seq序号位为Z随机值。客户端收到服务器发送的断开连接报文后，客户端退出**FIN-WAIT-2**阶段，进入**TIME-WAIT**阶段
4. 客户端进入TIME-WAIT阶段时，会立即发送最后一个确认报文，ACK标记位为1，ack确认序号位**Z+1**。服务器收到客户端发送的确认报文后，退出**LAST-ACK**阶段，进入**CLOSED**阶段
5. 如果这个2MSL时间内服务器没有再一次发送FIN报文的话，那个2MSL时长 后，客户端退出TIME-WAIT阶段，进入**CLOSED**阶段

### ！！！为什么是4次挥手，不是3次

答：TCP建立连接的时候之所以是3次握手，是因为第二次握手的过程中，s端发送给c端的TCP报文（SYN+ACK）一起发送，其中**ACK报文是用来应答的，SYN报文是用来同步的**。而释放连接时，之所以需要4次是因为ACK确认接收报文，FIN释放连接报文是分别由第二次和第三次分开发送。

+ 主动断开方发送FIN时，被动断开方要回复ACK，意思是“我收到你的FIN了”；
+ 主动断开方发送FIN并不意味着立即关闭TCP连接，而是告诉对方自己没有更多的数据要发送了，只有当对方发完自己的数据再发送FIN后，才意味着关闭TCP连接；
+ 被动断开方收到FIN并回复ACK后，此时TCP处于“半关闭”状态，为保证被动断开方可以继续发送数据，所以第二个FIN并不会伴随ACK发送，所以比连接时多一个报文段。

### ！！！为什么建立连接的时候ACK+SYN是一起发送？释放连接时ACK和FIN是分开发送

1. 建立连接时：被动方s端从LISTEN阶段进入SEND_RECV阶段不需要做什么准备，可以直接发送ACK+SYN
2. 断开连接时：被动方s端，突然收到c端断开连接的请求，**此时可能还有数据还要处理，所以不能立即断开**，需要等到数据处理完毕后，再告诉c端可以完全断开。所以先回了ACK确认报文（告诉c端你要断开的请求我收到了），等到数据处理完毕后，再发送FIN释放连接报文，断开了解

### ！！！TIME_WAIT存在原因

+ 2MSL：1MSL，为1段TCP报文再传输过程中最大生命周期
+ 等待2MSL的根本原因，为了实现TCP全双工可靠的释放
  + 在第4次挥手时，C端向S端发送最后的ACK，也可能会丢失，导致S端收不到该确认报文。当S端在1MSL时间内没有接收到C端发送的ACK确认报文时，S端会再次向C端**重新发送FIN报文（重传机制）**。C端收到FIN报文后，会重新发送最后一个ACK报文，直到S端能成功的接收到ACK报文，连接才真正的断开。
+ 为了使就得数据包在网络中因过期而消失（防止被动方收到主动方发送的过期数据）
  + 先假设没有TIME_WAIT状态的限制，如果当前有一个TCP连接（local_ip，local_port，remote_ip，remote_port）在断开的同时，以相同的4元组去建立新的连接。那么TCP协议栈就无法区分前后两条tcp连接是不同的，在它看来就一条连接。导致前一条TCP已经关闭的连接发送出去的数据，，通过之后的新连接，仍然可以发送给s端

### ！！！TIME_WAIT过多

+ 主动调用close的一方，会发送最后一个ACK后，进入TIME_WAIT状态
+ TIME_WAIT过多导致的原因是：**大量的短连接**。许多短连接正在被快速的创建和关闭，由于关闭需要等到2MSL时长，看你导致出现大量的TIME_WAIT
+ 危害：
  + 每个套接字都要消耗一个fd，缓冲区等等。浪费资源（类似僵尸资源）
  + 如果没有设置地址/端口复用，在TIME_WAIT时期内不能用该地址
+ 如果服务端出现大量的time_wait。那么可能是业务逻辑上面判断出现了什么问题导致主动调用close。解决方法：socket设置重用

### ！！！CLOSE_WAIT过多

+ 被动方接受到close请求，然后发送ACK后进入CLOSE_WAIT。
+ 原因：被动方没有及时调用close。代码逻辑有问题，处理异常情况的时候没有调用close函数

### ！！！半关闭

+ 半关闭：一个套接字内部由内核借助两个缓冲区实现（一个读缓存，一个写缓存），当处于半关闭状态的时候：**只关闭了写缓存，但是连接还是存在的，还能接收数据。**
+ 半关闭状态：主动发起关闭的一方不能再发送数据了，只能接受数据

### ！！！TCP状态转换图

TCP状态转换图：![TCP状态转换图](/网络/images/TCP状态转换图.jpg)

> **建立连接**：
>
> + 主动发起连接请求端：<u>**CLOSE**</u> --》 发送SYN --》 <u>**SEND_SYN**</u> --》 接收ACK，SYN --》 <u>**SEND_SYN**</u> --》 发送ACK --》<u>**ESTABLISHED**</u>（数据通信状态）
>
> + 被动接收连接请求端：<u>**CLOSE**</u> --》 <u>**LISTEN**</u> --》 接收SYN --》<u>**LISTEN**</u> --》 发送ACK，SYN --》SYN_RCVD --》接收ACK --》 <u>**ESTABLISHED**</u>（数据通信状态）

> **断开连接：**
>
> + 主动关闭连接请求端：<u>**ESTABLISHED**</u>（数据通信状态） --》 发送FIN --》 <u>**FIN_WAIT1**</u> --》 接收ACK --》<u>**FIN_WAIT2**</u>（半关闭）--》接收端发送FIN --》<u>**FIN_WAIT_2**</u>（半关闭）--》回发ACK --》<u>**TIME_WAIT**</u>（只有主动关闭连接方，才会有该状态）--》等待**2MSL**时长 --》<u>**CLOSE**</u>
> + 被动关闭连接请求端：<u>**ESTABLISHED**</u>（数据通信状态）--》接收FIN --》 <u>**ESTABLISHED**</u>（数据通信状态）--》发送ACK --》<u>**CLOSE_WAIT**</u> --》发送FIN --》<u>**LAST_ACK**</u> --》接收ACK --》<u>**CLOSE**</u>

### 产生RST的场景

+ RST表示复位，用来异常的关闭连接。发送RST包关闭连接时，不必等到缓冲区的数据包全部发送（将直接丢弃缓冲区中的数据）。接收端收到RST包，也不必发送ACK包来确认，直接就关闭
+ 客户端去连接一个未打开的端口：会向该客户端发送一个RST
+ 请求超时：才用SO_RCVTIMEO 选项设置套接字选项，超时后，会发送RST
+ 提前关闭：B阻塞接收4096个数据后，调用close。A向B阻塞发送5000个数据，此时B接收到4096个字节后，就会调用close，将返回接收到4096个字节的ACK给A，并再发送RST给A，断开连接
+ 向一个已经关闭的套接字发送数据：A与B建立连接后，此时B调用close关闭fd，A再向B的fd发送数据，A会受到RST

### 长连接和短连接

|      | 短连接                                                         | 长连接                                                           |
| ---- | -------------------------------------------------------------- | ---------------------------------------------------------------- |
| 概念 | 客户端和服务器每进行一次操作，就要建立一次连接，任务执行完断开 | 客户端和服务器建立连接，长期保持连接。只有当所有任务执行完才断开 |
| 缺点 | 请求频繁时，频繁的创建和销毁消耗资源，导致产生大量的TIME_WAIT  | 占用连接的资源，资源个数优先                                     |
| 优点 |                                                                | 省去较多的TCP建立和关闭带来的浪费                                |
| 场景 | Web网址的HTTP服务                                              | 操作频繁，点对点通信，连接数不能太多                             |

### 保活：心跳包机制

+ 方案一：TCP套接字选项SO_KEEPALIVE。系统每两个小时定时发送探测消息
+ 方案二：应用层，自定义心跳包机制。
  + client每隔一段时间向server发送探测包，并启动定时器。
  + server接收到包后，进行回应
  + client在规定时间内没有收到回应，则认为server挂了。否则认为server正常，删除超时定时器。

### 滑动窗口

+ 目的：控制发送端的发送速度，防止发送数据过快，导致接收方接收不过来
+ 原理：接收方每次收到数据，向发送方返回ACK的时候，会携带一个接收窗口大小的字段

### 为什么需要滑动窗口

答：如果是一发一应的模型，一次发送之后需要等待对方的应答才能发送下一次，那这种模型发送性能极差。使用滑动窗口就可以大大提高发送的性能，在对端窗口大小内都能发送

### 零窗口和坚持定时器

+ 当接收缓冲区满时，B在回复ACK的时候，会告诉A自己的接收窗口rwnd=0了，此时A接收到窗口大小为0后，就会挺尸发送数据。（此时启动坚持定时器）
+ B发送给A窗口为0的确认包后，A就不会再给B发送数据。等待一段时间后，B缓冲区有空间了，就会向A发送一个报文，通知A现在有发送数据了。但是该报文再传输过程中一旦丢失，就会出现下面“双方死等现象”。A等待B通知它发送数据，B发送报文后等待A发送数据。**为此就坚持定时器就是解决该场景**
+ 坚持计时器：在A接收到一个窗口为0的报文后，将每隔一段时间，发送给B探测报文，即探测接收窗口大小的报文，一旦接收窗口大小不为0，就可以继续发送数据

 ### 拥塞控制

+ 目的：通过拥塞窗口cwnd来防止过多的数据注入到网络中，导致网络拥堵。和滑动窗口不同，解决的问题也不同
+ 原理：通过“发送方”的拥塞窗口cwnd与门限值ssthresh大小比较，根据比较结果，调用响应的拥塞控制算法

### 慢启动

+ 一开始建立连接后，将cwnd设为1，cwnd是指数增长的，直到cwnd增大到门限值ssthresh，就会进入拥塞避免阶段

### 拥塞避免

+ cwnd是线性增长的（“加法增大”），直到发生【网络超时】，就会重新开启慢启动。此时门限值ssthresh设置窗口的1/2，cwnd设置为1，继续执行慢启动

### 快重传

+ 当发送方收到同一个包的连续3次重复确定时（说明网络丢包），此时不用等待网络超时，直接重传数据给接收方

### ！！！TCP/UDP的区别

| TCP                                                                | UDP                  |
| ------------------------------------------------------------------ | -------------------- |
| 面向连接（可靠）                                                   | 面向无连接（不可靠） |
| 流式协议（粘包拆包问题）                                           | 报文（有边界）       |
| 处理和传输速度较慢                                                 | 处理/传输速度较快    |
| 具有可靠性（头部校验，滑动窗口，拥塞控制，超时重传机制，应答机制） | 不可靠，需要自己保证 |

### ！！！TCP可靠，为什么很多项目仍选择UDP

答：UDP不考虑数据的可靠性，传输效率更快，尽最大可能传递数据。很多项目不需要太考虑可靠性，但是需要很快的传输速度，例如音视频。如果需要很快的传输速度，又需要数据可靠，那需要应用层自己实现一套可靠的机制

### ！！！TCP是可靠的，为什么通信会丢包

+ TCP协议本身是保证传输的数据完整性不会丢数据的。
+ 如果通信中发现缺少数据或者丢包，那么，最大的可能在于程序发送的过程或者接收的过程出现问题。
+ 例如：A给B以极高的频率调用send发送数据，那么就可能在send时发生错误（原因可能很多：缓冲区溢出，多线程同步等），如果send发生错误却没有重发数据，最终会导致B收到数据小于A发送的数据
+ 这种现象本质上不是丢包，也不是丢数据。而是因为程序处理有错误，导致有些数据没有成功被发送出去

### ！！！UDP可靠性设计方案：KCP/RUDP

**主要思想参考TCP可靠性方案**

1. 数据包切分（MTU）：每次调用sendto，只能发送一个报文，当发送的报文超过MTU时，就会报错或者丢包。对于一个很长的UDP数据报，要进行切分多个端进行传输
2. 序列号seq（保证不失序）：给每个数据包都添加一个应用层header，header中包含一个字段序列号seq，接收端接收到数据后，使用该序列号对数据进行排序，保证数据有序
3. 发送与确认机制（保证不丢失数据）：发送端每次发送一个数据，都要等到接收方回复一个ACK
4. 重传机制（保证不丢失数据）：发送完数据后，要启动一个超时定时器，一旦超时就要重新发送数据
5. 滑动窗口/流量控制（保证尽可能合理的多发数据）：为了加速数据的发送效率，参考TCP的滑动窗口，允许发送端一次可以发送滑动窗口大小的字段给接收方。为了避免发送数据较快，引入流量控制，根据对方的接收窗口大小，调整发送数据的速度



### listen的第二个参数

```
listen(int fd, int backlog)
```

TCP通过三次握手建立连接，采取两个队列：请求队列，已连接队列

第二个参数：请求队列的大小，当请求数超过长度时，将会被丢弃

1. 未连接队列大小设置为100，此时有1000个连接请求，那么只处理前100个，后面900个丢弃
2. 当连接建立成功后，将从请求队列中剔除，加入到已连接队列

### 连接建立是在accept函数吗

答：socket客户端连接上服务器是在listen之后而非accept之时

1. 只要服务器开启listen，客户端调用connect时，就连接成功（三次握手成功）
2. accept只是将fd从连接队列中移除
3. 服务器在listen调用之后sleep，没有调用accept。客户端仍然可以调用send发送数据，也会发送成功

### close/shutdown，引用计数

+ fd引用计数：对于每个打开的套接字fd，它都是有一个引用计数，当有多个继承打开该fd时，引用计数+1

+ close：会关闭套接字，但是调用close时，有其他进程共享该套接字，那么该连接仍是打开的，该连接在其他进程可以进行读写。只是本进程已经关闭。对应的fd引用计数-1。只有当fd引用计数清0时，才会真正关闭fd
+ shutdown：会将fd引用计数清零，其他进程共享的fd也无法使用。
+ shutdown相当于将“主动关闭连接”这件事分成两步：
  + A调用shutdown关闭写方向，保留读方向。B调用read将返回0，然后B手动调用shutdown关闭读方向，保留写方向。（此时A进入半关闭状态）
  + B发送完数据后，调用shutdown关闭写方向。A调用read将返回0，然后A手动调用shutdown关闭读方向。（此时A，B进入关闭状态）

### UDP/TCP的connect区别

1. TCP的connect会引起三次握手，UDP不会
2. 由于UDP是无连接的，在调用connect时其实没有向外发包，只是在协议栈中记录了该状态
3. UDP调用了connect后，也可以用send和write，read和recv。当然也可以调用sendto，recvfrom
4. UDP可以调用多次connect，TCP只能调用一次connect

### UDP多次调用connect目的

1. 指定一个新ip，port连接
2. 断开和之前的ip，port连接：将connect第二个参数中的sin_famliy设置成AF_UNSPEC
3. 提高发送数据的效率
   1. 普通的UDP发送两个报文内核做了：建立连接->发送报文->断开连接->建立连接->发送报文->断开连接
   2. 采用connect方式的UDP发送两个报文内核做了：建立连接->发送报文->发数码管纳帕纹
4. 异步ICMP错误不会反悔给unconnect的udp套接字，调用connect后，可以接收到异步ICMP的错误



### select/poll/epoll的区别

|            | epoll            | select                                                                                             | poll                                                                                               |
| ---------- | ---------------- | -------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |
| 时间复杂度 | O(1)             | 当有 I/O 时间发生时，不能知道是哪个 fd 触发了，只能无差别的轮询所有 fd。 O(N)                      | 当有 I/O 时间发生时，不能知道是哪个 fd 触发了，只能无差别的轮询所有 fd。 O(N)                      |
| 底层实现   | 红黑树+就绪链表  | 数组（poll 与 select 没有本质上的区别， 但是它没有最大连接数的限制，原因是它是基于链表来存储的。） | 链表（poll 与 select 没有本质上的区别， 但是它没有最大连接数的限制，原因是它是基于链表来存储的。） |
| 缺点       | 消耗的资源比较多 | 最大连接数有上限<br>需要对fd集合进行轮询扫描<br>需要将fd集合从内核和用户空间来回拷贝               | 需要对fd集合进行轮询扫描<br/>需要将fd集合从内核和用户空间来回拷贝                                  |

### select

+ select：是最初解决IO阻塞问题的方法。用结构体fd_set来告诉内核监听多个文件描述符，该结构体被称为描述符集。由数组来维持哪些描述符被置位了。对结构体的操作封装在三个宏定义中。通过轮寻来查找是否有描述符要被处理。

+ 存在的问题：

  1. 内置数组的形式使得select的最大文件数受限与FD_SIZE；

  2. **每次调用select前都要重新初始化描述符集，将fd从用户态拷贝到内核态，每次调用select后，都需要将fd从内核态拷贝到用户态；**

  3. 轮寻排查当文件描述符个数很多时，效率很低；

### poll

+ poll：通过一个可变长度的数组解决了select文件描述符受限的问题。数组中元素是结构体，该结构体保存描述符的信息，每增加一个文件描述符就向数组中加入一个结构体，结构体只需要拷贝一次到内核态。poll解决了select重复初始化的问题。轮寻排查的问题未解决。

### epoll底层实现原理

+ 底层实现原理：
  + 当内核初始化epoll时，会开辟一块内核高速cache区，用于安置我们监听的socket，这些socket会以红黑树的形式保存在内核的cache里，以支持快速的查找，插入，删除．同时，建立了一个list链表，用于存储准备就绪的事件．所以调用epoll_wait时，在timeout时间内，只是简单的观察这个list链表是否有数据，如果没有，则睡眠至超时时间到返回；如果有数据，则在超时时间到，拷贝至用户态events数组中．
  + 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。
+ epoll_create：创建一个 epoll 文件描述符，底层同时创建一棵红黑树 and 一个就绪链表 rdlist。红黑树存储了所有监控的文件描述符；就绪链表存储就绪文件描述符
+ epoll_clt：
  + 删除：将文件描述符从红黑树上摘除
  + 插入：先查看红黑树是否有该 fd：如果有，不再插入；如果没有，插入。（epoll 使用“事件”的就绪通知方式，通过 epoll_ctl 注册 fd，一旦该 fd 就绪，内核就会采用类似callback 的回调机制来激活该 fd，epoll_wait 便可以收到通知）
+ epoll_wait：只是从就绪链表中取出元素，将该元素上的事件复制到用户态区间（使用 mmap 提高效
  率）

### epoll水平触发和边缘触发

+ 水平触发：只要高电平 or 低电平时，才会触发。即：有数据可读时，就会一直触发（事件没处理完，不会从激活链表中摘除，会一直触发，直到事件处理完，才移除激活队列）
+ 边缘触发：只有电平发生变化时，才会触发（事件来到时，只触发一次，就从激活链表中摘除）
+ 举例：一个管道收到了 1kb 的数据，epoll 会立即返回，此时读了 512 字节数据，然后再次调用 epoll。
  + 如果是水平触发的，epoll 会立即返回，因为有数据准备好了
  + 如果是边缘触发的不会立即返回，因为此时虽然有数据可读，但是已经触发了一次通知，在这次通知到现在还没有新的数据到来，直到有新的数据到来时，epoll 才会返回。
+ 水平触发注意：对于读事件，如果用水平触发不用担心数据有没有读完，因为下次 epoll 返回时，没有读完的 socket 依然会被返回。但是要注意这种模式下的写事件，因为是水平触发，每次socket 可写时，epoll 都会返回，当我们写的数据包过大时，一次写不完，要多次才能写完或者每次 socket 写都写一个很小的数据包时，每次写都会被 epoll 检测到，因此长期关注 socket写事件会无故 cpu 消耗过大甚至导致 cpu 跑满，所以在水平触发模式下一般不关注 socket 可写事件而是通过调用 socket write 或者 send api 函数来写 socket
+ 水平触发在效率上是没有边缘触发高的，因为每一次 socket 读或写可能被返回两次甚至多次，所以有时候也会用到边缘触发。但是这种模式下在读数据的时候一定要注意，因为如果一次没有把数据读完，且在 socket 没有新的数据可读时，epoll 就不回返回了，将导致数据没有读取完。、
+ 对于边缘触发，必须：使用非阻塞 IO 和  while 循环一次性读写完全部数据

### EPOLL_ONESHOT 和 边缘触发

+ 解释EPOLL_ONESHOT：由上面我们知道，采用 ET 模式，为了保证数据能够全部读完，采用 while 循环的方式！但是，如果在 while 循环读取过程中，此时有新的事件到达，很可能触发了其他线程来处理这个 socket，这就发生了错乱！
+ 解决方案：一旦事件触发后，就调用 epoll_clt 将事件从红黑树上摘除，直到处理完该事件，再调用 epoll_clt 将该事件添加到红黑树。这样，就可以保证同一个 fd 事件在同一时刻只能触发一次！

### TCP 和 IP 什么区别

#### IP协议：

IP（InternetProtocol）协议的英文名直译就是：因特网协议。从这个名称我们就可以知道IP协议的重要性。在现实生活中，我们进行货物运输时都是把货物包装成一个个的纸箱或者是集装箱之后才进行运输，在网络世界中各种信息也是通过类似的方式进行传输的。IP协议规定了数据传输时的基本单元和格式。如果比作货物运输，IP协议规定了货物打包时的包装箱尺寸和包装的程序。除了这些以外，IP协议还定义了数据包的递交办法和路由选择。同样用货物运输做比喻，IP协议规定了货物的运输方法和运输路线。

IP：网路层协议

#### TCP协议：

我们已经知道了IP协议很重要，IP协议已经规定了数据传输的主要内容，在IP协议中定义的传输是单向的，也就是说发出去的货物对方有没有收到我们是不知道的。就好像8毛钱一份的平信一样。那对于重要的信件我们要寄挂号信怎么办呢？TCP协议就是帮我们寄“挂号信”的。TCP协议提供了可靠的面向对象的数据流传输服务的规则和约定。简单的说在TCP模式中，对方发一个数据包给你，你要发一个确认数据包给对方。通过这种确认来提供可靠性。

TCP:面向连接的传输层协议

+ tcp在ip上一层。如果用一个包的角度来看的话，tcp被ip报包住。tcp提供端口号，序列等来实现面向连接的服务。而ip提供ip地址来进行数据报在网络中的传送，只提供尽可能传送的服务

### 拔掉网线后， 原本的 TCP 连接还存在吗？

针对这个问题，要分场景来讨论：

- 拔掉网线后，有数据传输；
- 拔掉网线后，没有数据传输；

#### 拔掉网线后，有数据传输

在客户端拔掉网线后，服务端向客户端发送的数据报文会得不到任何的响应，在等待一定时长后，服务端就会触发**超时重传**机制，重传未得到响应的数据报文。TCP 的数据报文具体重传15次，超过15次则认为异常。还有一个最大超时时间，如果超过最大超时时间也认为异常，断开连接。**在重传报文且一直没有收到对方响应的情况时，先达到「最大重传次数」或者「最大超时时间」这两个的其中一个条件后，就会停止重传，然后就会断开 TCP 连接。**

>  **如果在服务端重传报文的过程中，客户端刚好把网线插回去了**，由于拔掉网线并不会改变客户端的 TCP 连接状态，并且还是处于 ESTABLISHED 状态，所以这时客户端是可以正常接收服务端发来的数据报文的，然后客户端就会回 ACK 响应报文。
>
> 此时，客户端和服务端的 TCP 连接依然存在的，就感觉什么事情都没有发生。

> **如果如果在服务端重传报文的过程中，客户端一直没有将网线插回去**，服务端超时重传报文的次数达到一定阈值后，内核就会判定出该 TCP 有问题，然后通过 Socket 接口告诉应用程序该 TCP 连接出问题了，于是服务端的 TCP 连接就会断开。
>
> 而等客户端插回网线后，如果客户端向服务端发送了数据，由于服务端已经没有与客户端相同四元祖的 TCP 连接了，因此服务端内核就会回复 RST 报文，客户端收到后就会释放该 TCP 连接。
>
> 此时，客户端和服务端的 TCP 连接都已经断开了。

#### 拔掉网线后，没有数据传输

如果**没有开启** TCP keepalive 机制，在客户端拔掉网线后，并且双方都没有进行数据传输，那么客户端和服务端的 TCP 连接将会一直保持存在。

而如果**开启**了 TCP keepalive 机制，在客户端拔掉网线后，即使双方都没有进行数据传输，在持续一段时间后，TCP 就会发送探测报文：

- 如果**对端是正常工作**的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
- 如果**对端主机崩溃，或对端由于其他原因导致报文不可达**。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**。

所以，TCP 保活机制可以在双方没有数据交互的情况，通过探测报文，来确定对方的 TCP 连接是否存活

在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为默认值：

- tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制
- tcp_keepalive_intvl=75：表示每次检测间隔 75 秒；
- tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。

### 客户端宕机， 原本的 TCP 连接还存在吗？

**客户端宕机这件事跟拔掉网线是一样**无法被服务端的感知的，所以如果在没有数据传输，并且没有开启 TCP keepalive 机制时，，**服务端的 TCP 连接将会一直处于 ESTABLISHED 连接状态**，直到服务端重启进程。

所以，我们可以得知一个点。在没有使用 TCP 保活机制，且双方不传输数据的情况下，一方的 TCP 连接处在 ESTABLISHED 状态时，并不代表另一方的 TCP 连接还一定是正常的。

### 杀死进程客户端， 原本的 TCP 连接还存在吗？

杀死客户端的进程后，**客户端的内核就会向服务端发送 FIN 报文，与客户端进行四次挥手**。

所以，即使没有开启 TCP keepalive，且双方也没有数据交互的情况下，如果其中一方的进程发生了崩溃，这个过程操作系统是可以感知的到的，于是就会发送 FIN 报文给对方，然后与对方进行 TCP 四次挥手。

### 快速重传为了解决什么问题

快速重传解决超时重传时间周期过长问题。对同一个包的重传时间都会翻倍，导致后期超时重传周期过长

### TIME_WAIT快速回收和重用

端口全部用完，time_wait会等到2MSL时长才回收端口，会导致没有端口使用

如果能保证以下任意一点，一个TW状态的四元组(即一个socket连接)可以重新被新到来的SYN连接使用：

1. 初始序列号比TW老连接的末序列号大
2. 如果使能了时间戳，那么新到来的连接的时间戳比老连接的时间戳大

打开重用 net.ipv4.tcp_tw_reuse = 1

打开快速回收 net.ipv4.tcp_tw_recycle = 1

### TCP如何优雅的断开连接

优雅关闭：其实就是正常的四次挥手
非优雅关闭：向对端发送一个RST报文直接进入CLOSED状态

服务器为了避免太多TIME_WAIT的关闭方式：
1.保证由客户端主动发起关闭
2.关闭的时候使用RST的方式
3.对处于TIME_WAIT状态的TCP允许重用
一般我们当然最好是选择第一种方式，实在没有办法的时候，我们可以使用SO_LINGER选择第二种方式，使用SO_REUSEADDR选择第三种方式

设置 close()关闭 TCP 连接时的行为。当 socket 发送缓冲区有数据残留时，

1. 默认的行为，一直等待数据发送给对方，数据缓冲区中没数据后，才 close

2. 可以设置的行为

   1. 立即关闭该连接。直接丢弃发送缓冲区中的数据，并向向对方发送 RST 标记，来关闭该连 接。主动关闭的一方将跳过 TIMEWAIT 状态，直接进入 CLOSED 状态。
   2. 给关闭连接设置一个超时时间。[1] 如果超时时间没到，且缓冲区中的数据被全部发送，内核将用 FIN/ACK/FIN/ACK 的方式关闭连接。[2] 如果超时时间到了，且缓冲区中依然有数据，则采用方式 a 的处理方式。
   3. SO_LINGER：可以设置优雅关闭

   ```
   struct linger {
   	int l_onoff;	// SO_LINGER是否开启，0关闭，1开启
   	int l_linger;	// 超时情况（0选择1，1选择2）
   };
   ```

### TCP 快速重传为什么是三次冗余 ACK，这个三次是怎么定下来的？

**两次duplicated ACK肯定是乱序造成的！**

**丢包肯定会造成三次duplicated ACK!**

https://www.zhihu.com/question/21789252

### 传输层与网络层的区别

传输层位于网络层之上，传输层协议为不同[主机](http://wenwen.soso.com/z/Search.e?sp=S主机&ch=w.search.yjjlink&cid=w.search.yjjlink)上运行的应用进程提供[逻辑](http://wenwen.soso.com/z/Search.e?sp=S逻辑&ch=w.search.yjjlink&cid=w.search.yjjlink)通信，而[网络层协议](http://wenwen.soso.com/z/Search.e?sp=S网络层协议&ch=w.search.yjjlink&cid=w.search.yjjlink)为不同主机提供逻辑通信。

> 网络层负责ip数据报的产生以及ip数据包在逻辑网络上的路由转发。
>
> 网络层只是根据网络地址将源结点发出的数据包传送到目的结点（点到点），其主要任务是：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。该层控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。具体地说，数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。

> 传输层提供端到端通信服务层次,提供可靠及非可靠连接。
>
> 传输层则负责将数据可靠地传送到相应的端口（端到端），传输层提供了主机应用程序进程之间的端到端的服务。传输层利用网络层提供的服务，并通过传输层地址提供给高层用户传输数据的通信端口，使高层用户看到的只是在两个传输实体间的一条端到端的、可由用户控制和设定的、可靠的数据通路。

### **SO_REUSEPORT**

> http://abcdxyzk.github.io/blog/2020/05/25/kernel-reuseport-history/
>
> https://simpleyyt.com/2017/06/25/how-ngnix-solve-thundering-herd/
>
> https://www.cnblogs.com/Anker/p/7071849.html
>
> https://blog.csdn.net/lyztyycode/article/details/78648798
>
> SO_REUSEPORT 该特性支持多个进程或者线程绑定到同一端口，提高服务器程序的性能，允许多个套接字 bind()以及 listen()同一个 TCP 或 UDP 端口，并且在内核层面实现负载均衡
>
> Ngnix 的 master 进程在创建 socket，`bind()`和`listen()`之后，`fork()`出多个 worker，worker 会 将这个 socket 加入 epoll 中，用`epoll_wait()`来处理事件，当有一个新的连接来的时候，所有 worker 都会被唤醒，这就是所谓的 **epoll 惊群**。
>
> 每个 worker 都有自己的 socket，这些 socket 都`bind`同一个端口。当新请求到来时，内核根据四元组信息进行负载均衡，非常高效。
>
> （1）、允许多个套接字bind()/listen()同一个tcp/udp端口。每一个线程拥有自己的服务器套接字，在服务器套接字上没有锁的竞争。
>
> （2）、内核层面实现负载均衡
>
> （3）、安全层面，监听同一个端口的套接字只能位于同一个用户下面。
>
> （4）、处理新建连接时，查找listener的时候，能够支持在监听相同IP和端口的多个sock之间均衡选择。

### SO_REUSEADDR

目的：当服务端出现timewait状态的链接时，确保server能够重启成功。

注意：**SO_REUSEADDR只有针对time-wait链接(linux系统time-wait连接持续时间为1min)，确保server重启成功的这一个作用。**
