### 介绍下iscsi远端盘项目

#### 1. 项目背景：

日常中我们使用的硬盘大多数都是本地硬盘，直接插到主机上面进行本地读写。
而我们分布式存储提供了远端硬盘的模式，可以通过网络进行远端IO读写，简单理解就是网盘。
而使用的网络通信协议是iscsi，我们基于iscsi实现了服务端，客户端由其他厂商实现。

#### 2. 工作内容：

分布式存储就需要面临解决三高问题：高性能（高IO吞吐量），高可用（保证业务不中断），高可靠（数据一致性）。

##### 2.1 第一个问题：高性能

我们使用了开源框架tgtd作为服务端，而它是单线程模型，当 CPU 跑满时性能指数不达标，无法实现高性能，所以我们要对其性能优化。
原生tgtd是单反应堆模型，现在改造成多反应堆提升网络通信性能。
也没有线程池，所有处理都在主线程处理。所以引进了IO线程池和共享请求队列，对IO进行异步处理。
而由单线程改成多线程后，势必会遇到并发控制问题。
我们使用了互斥锁，读写锁，条件变量解决问题。在使用锁过程中还遇到不可重入锁的问题。
对tgtd改进完的成果是：读性能提升40%，写性能提升10%

##### 2.2 第二个问题：高可用

tgtd是基于TCP/IP进行网络传输的。我们一个物理网口坏了或者主机宕机了引起物理IP不可用，导致业务中断。所以我们需要一个保活机制，保证IP一直可用。
我们引入了Keepalived组件，该组件提供了虚拟IP的机制，为集群每台主机配置虚拟IP。
当某台主机不可用时，则该主机上的虚拟IP会被其他节点接管。从而保证IP一直可用

##### 2.3 第三个问题：高可靠

高可靠 是由我们的后台存储模块负责的。tgtd只是将io请求转发给后端存储模块，不负责副本一致性问题。

##### 2.4 最后说下：负载均衡

项目初期并没有负载均衡这个功能，每个主机接入的流量不均衡，导致数据倾斜。
为了充分利用好每个主机资源，使流量尽可能均摊到每个主机，才实现了负载均衡功能。
负载均衡算法中有轮询，随机，加权轮询，平滑加权轮询算法，最少活跃数等。
基于服务情况，最后选择了最少活跃数算法。重定向到硬盘访问个数最少的主机。
（情况：我们这个是保持长连接进行请求处理。当某些主机挂了，客户端重新发起连接，如果使用轮询算法的会还是会不均衡）

### 哪个线程监听fd的读写事件

由网络线程去监听fd的读写事件

### 哪个线程去唤醒io线程去共享队列取数据去处理

网络线程监听到有读事件后，从fd上读取数据并且扔到共享队列之后就唤醒下面的io线程去取数据工作。

### 你了解nfs协议吗

NFS是NetworkFileSystem的简写,即网络文件系统,网络文件系统是FreeBSD支持的文件系统中的一种，也被称为NFS.NFS允许一个系统在网络上与它人共享目录和文件。通过使用NFS，用户和程序可以像访问本地文件一样访问远端系统上的文件。

### 除了tgtd还有其他开源框架吗

还有其他的开源框架，但是没有了解

### 你们的连接建立频繁吗

建立连接不频繁，客户端和服务端之间是长连接保持通信的

### cpp的线程池是怎么实现的

https://zhuanlan.zhihu.com/p/477803392

### cpp的共享指针他是线程安全的吗

共享指针里面的引用计数是线程安全的，但是他引用的对象并不是线程安全的。

### 共享指针他一般用在哪个场景

### unique_ptr你了解吗

实现独占式拥有或严格拥有概念，保证同一时间内只有一个智能指针可以指向该对象。它对于避免资源泄露(例如“以new创建对象后因为发生异常而忘记调用delete”)特别有用。

### unique_ptr他怎么传递，不同对象之间的传递

### 介绍下epoll的reator模型

epoll反应堆：创建epollfd->监听读事件->epoll_wait返回->执行read（可读）->将读事件从树上摘下，将写事件重新挂到树上->epoll_wait返回->执行write（可写）->将写事件从树上摘下，将读事件重新挂到树上。。。（循环如此）

### 你知道proreator吗

https://blog.csdn.net/iteye_21199/article/details/82452615?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-82452615-blog-94679658.pc_relevant_multi_platform_whitelistv3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-1-82452615-blog-94679658.pc_relevant_multi_platform_whitelistv3&utm_relevant_index=2

### 介绍下你的磁盘策略模块

+ 背景：产品类似于 VMWare，可以创建虚拟机（虚拟 PC 机），虚拟机的每个磁盘都能配置不同的属性（副本数，性能高低，聚合离散等）。为了统一管理属性，方便用户选择对应的属性，将属性进行配置化。可以根据不同的使用场景，配置不同的磁盘策略。
+ 工作职责：
  1. Zookeeper：实现了全局磁盘策略统一配置管理，并且使用 watcher 机制的注册与通知机制，回调 Redis
     缓存增删改处理。
  2. Redis: 采用 Redis 作为缓存数据库，相较于 Zookeeper 具备更好的性能。采用“延迟双删”保障了缓
     存与数据的一致性。
  3. 分布式锁: 使用 Redis 的 set(key,val,nx,px)设置分布式锁。防止一个服务对策略进行修改，而另外个服
     务却使用策略创建虚拟机的，导致造成创建出来的磁盘跟原策略属性不一样。
  4. 发布订阅: 使用 Redis 的发布订阅机制实现异步消息处理。当后端数据面创建磁盘成功后发送消息，通
     知前端管理面可以进行后置处理。管理面从而保存虚拟机和策略之间的映射关系。
+ 项目结果：从 Beta 版发布就收到用户的好评，方便对虚拟机全局属性配置设置，解决了客户操作
  繁琐的问题，提升易用性

### 为什么要使用zk和redis

一开始只有zk，没有redis。zk就是作为一个配置中心，提供读写配置。但是当高并发场景，不止是我这个服务跟zk有交互，还有其他服务跟zk也有交互。这样导致zk的读写压力过大，性能不好。所以才引入了redis

### zk和redis之间的数据一致性怎么保证

采用延迟双删机制：先删除redis缓存再删除zk，删除zk节点会触发一个watch的机制，再去删除redis

### 你们为什么不用etcd呀

因为项目启动之前已经有用zk了，zk和etcd都是一样的，强一致性的分布式组件。所以没必要在引进etcd

### 对zk的访问量多吗

没有具体的监控指标，所以也不太清楚

### iscsi远端盘压测情况如何

改造完后，读性能提升40%，写性能提高10%

### 块存储你有了解吗？（我介绍了我们glfs的底层架构）

后端存储整体架构，有点像是栈型结构，是由多层模块入栈构造。入栈调用本层的入口函数，出栈则调用cbk函数

nfs层（接入层） -> 回收站 -> iostat层 -> 分片条带层 -> route层 -> afr层（副本一致性）-> vclient层 ->分层 -> brick层

+ 接入层：对外提供访问接口。目前有两种接入方式（glfs-api，nfs文件系统）
+ 回收站：文件删除后并不会正常删除，而是先进入到回收站，通过删除策略进行删除
+ iostat层：统计当前读写IO的时延，时延较高则会告警
+ 分片条带层：有点想kafka的分区。就说将一个数据块切成各小份，进行并行读写，提高性能。
+ route层：查询文件的副本具体落在哪几个硬盘上面
+ afr层：用来做副本一致性处理的。一般情况就2个数据副本，1个仲裁副本。这种情况下只要有2个副本写成功都算成功。有一个写失败的话，其他两个好副本都会指控这个坏副本，被指控的副本不可读写。但是我们会通过修复机制进行对坏副本进行维修。有热修和冷修，冷修就是定期去扫描全盘文件，哪些有坏副本，选择部分进行修复。热修就是：对当前有数据读写的副本进行修复。当有读发现是坏副本，则进行修复。如果一个副本修复失败次数过多，那么就会触发重建机制。放弃这个副本，通过好副本重建一个副本到新硬盘上。
+ vclient层：用于跟所有的brick进行通信，brick就是一个个硬盘，存放真是数据。每个硬盘都有一个brick进程。
+ 分层：保存一些热数据，提高读写性能
+ brick层：最后数据真实落盘的地方，最后调用的是posix的接口进行读写操作

### 分片为什么能提高性能

因为他会将一个大块的写拆分成多个小块读写，并行发到每个磁盘上面去读写入，提高了效率。

### 如果1个4k数据，是一次性写4k数据快还是分成4次1k数据写快

一次性写4k数据比较快，分片也会根据要写入的大小是否要拆分

### 条带的作用

### 在高并发场景下如何实现高性能的大文件和小文件的快速读写。

http://t.zoukankan.com/-wenli-p-13380616.html

小文件采用零拷贝、大文件采用异步io+直接io

### 讲下raft共识算法

raft共识算法就是三个主要模块：leader选举，数据同步和分区共识

> leader选举
>
> + 所有节点都有一个定时器（每个节点的定时器都是随机值）。该定时器表示收到Leader的心跳包。
> + 如果在定时器内没有收到Leader的心跳包，则自己变成Candidate状态，参与竞选Leader。（选举超时，自己当成候选者）
> + 然后所有任参与选票，候选者的投票只会投给自己，其他人则投给某个候选者（可能出现多个候选者情况）
> + 如果自己的票数过半，则成为新的Leader。
> + 如果此时有人跟自己一样同时竞选Leader，那么这两个人都会在起一个定时器（随机值），进行在一轮的选票

> 数据同步
>
> + 当leader接收到请求后，将该请求写到自己的日志中
> + leader将该请求发送给其他节点，要求其他节点也写到日志中
> + 其他节点将请求写到日志后，回一个ack给leader
> + leader统计有过半节点回复了ack后，将请求数据更新
> + leader发送请求给其他节点，要求其他节点将数据更新

> 分区共识
>
> + 集群5个节点，一开始1个leader和4个Follower
> + 然后网络出现问题，把1个leader和1个Follower划分为一个区A。而其他三个节点为另外一个区B
> + 另外三个节点为一个区A后，进行重新选举leader
> + 有个clientA向分区A发送请求，但是这个时候分区只有两个节点，没有超过半数所以该请求无效
> + 有个clientB向分区B发送器请求，有过半节点同意该请求，所以请求有效，更新数据
> + 当网络恢复后，出现了两个leaderA和leaderB。因为leaderB是新一代的leader，所以leaderA放弃做leader。并且将分区B的数据更新到分区A的每个节点上
> + 问题：一致性并不一定代表完全正确性！三个可能结果：成功，失败，unknown

### leader选举中定时器的时间为什么要随机

因为如果时间都一样的话，会出现两个节点同时到达时间并发起投票选举。那么这个时候可能会有两个节点的票数一样，又要重新发起选举，以此类推。这样会增大选举时间。

### term的作用是什么

简单理解的话，leader的编号。

在Raft协议中，将时间分成了一些任意长度的时间片，称为term，term使用连续递增的编号的进行识别

每一个term都从新的选举开始，candidate们会努力争取称为leader。一旦获胜，它就会在剩余的term时间内保持leader状态，在某些情况下(如term3)选票可能被多个candidate瓜分，形不成多数派，因此term可能直至结束都没有leader，下一个term很快就会到来重新发起选举。

term也起到了系统中逻辑时钟的作用，每一个server都存储了当前term编号，在server之间进行交流的时候就会带有该编号，如果一个server的编号小于另一个的，那么它会将自己的编号更新为较大的那一个；如果leader或者candidate发现自己的编号不是最新的了，就会自动转变为follower；如果接收到的请求的term编号小于自己的当前term将会拒绝执行。

### 如果是主从切换的话，会发起一个not hut作用是啥？（不知道是不是not hut）

### cap你怎么理解

+ CAP定理又称CAP原则，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），最多只能同时三个特性中的两个，三者不可兼得。
+ **Consistency (一致性)**：更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致，这就是分布式的一致性。一致性的问题在并发系统中不可避免，对于客户端来说，一致性指的是并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。
+ **Availability (可用性)**：服务一直可用，而且是正常响应时间。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。
+ **Partition Tolerance (分区容错性)**：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响。

### 你们项目中有这个样的原则吗

有呀，zk就是cp

### 为什么zk是cp，怎么保证数据一致性

因为zk他的定位就是强一致性的分布式组件，他的数据一致性是通过只要半数节点以上在线再能进行写操作。如果半数以下就是只能读

### redis中它底层是hash表，那他的rehash机制是怎么做的

```cpp
struct dict {
	dictht ht[2];   // 2个哈希表: ht[0]正常情况下使用, ht[1]在rehash时使用
    int rehashidx;  // rehash索引 (没进行rehash时，该值为-1)
}
```

- 背景：哈希表中键值对的增加/减少，都可能导致rehash（为了使哈希表的 `负载因子` 维持在合理的范围内）：一般进行2倍扩充（算法导论中的平摊分析）

- rehash过程（渐进式rehash）

  - ht[1]分配空间，新建一个空的哈希表
  - rehash索引计数器（**rehash_index**），由-1变为0，表示rehash正式开始
  - 将ht[0]中的元素，rehash重新散列到ht[1]上
    - 每次一个(key,value)键值对rehash成功后，rehash索引计数器都+1
  - 当所有的ht[0]都rehash到ht[1]中后，ht[0]被清空，此时将ht[0],ht[1]**交换**，rehash结束，最后将rehash索引设为-1

+ 在rehash过程中，**新增加的(key,value)键值对**，怎么处理？
  - 答：会直接rehash到ht[1]上，这样做，会保证ht[0]只减不增

### redis你们的部署方式

我们部署方式是使用了codis集群。codis集群就是客户端分片集群

### 客户端sharding的方式，跨redis查询是如何实现

要查询某个key的时候，他会通过hash函数得到hash值，然后取模得到对应真正的redis节点进行查询

### 你们codis是主从的形式吗

是的，有主从形式

### redis的数据一致性是怎么保证

redis的数据一致性，他们主从之间数据同步有：增量复制和全量复制

> 全量复制：
>
> + 主节点通过bgsave命令fork子进程进行RDB持久化，RDB持久化过程是非常消耗CPU，内存（页表复制），硬盘IO
> + 主节点通过网络将RDB文件发送给从节点，对主从节点的带宽都会带来非常大的消耗
> + 从节点清空老数据，载入新RDB文件（载入过程是阻塞的，无法响应客户端命令）。
> + 如果从节点执行了bgrewriteaof，也会带来额外的消耗（aof复制也需要消耗资源）

> 增量复制：
>
> + 复制偏移量：执行复制的双方，主从节点分别会维护一个复制偏移量offset
> + 复制积压缓存区：主节点内部维护一个固定长度的，先进先出队列作为复制积压缓冲区，**当主从节点offset的差距超过换缓存区长度时，将无法进行增量复制，只能执行全量复制**

### 主挂了，从的话还是会有数据丢失，业务能接受吗

redis是会出现数据丢失的问题

### redis的持久化机制

redis提供了两种持久化机制：RDB和AOF

RDB：保存某一时间点之前的快照数据。恢复数据时直接将快照数据解析到内存

AOF：指所有命令行记录以redis命令请求协议的格式完全持久保存在aof文件中。恢复数据时，则根据redis命令依次执行。

混合持久化：可以通过aof-use-rdb-preamble yes开启。rdb和aof混合的方式，aof文件的头部添加了rdb的数据

### rdb他是直接dump内存数据吗

Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到 一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 

整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。

### rdb过程fork子进程方式有什么好处

1. 不会阻塞主线程的任何io操作，保证主线程能继续执行
2. fork会有读时共享，写时复制的机制，可以减少不必要内存拷贝

### time_wait是怎么产生的

time_wait是tcp四次挥手过程，主动方最后一次发出ACK包后，从fin-wait2状态切换成time_wait状态

### 有多长时间的time_wait，为什么

2MSL：1MSL，为1段TCP报文再传输过程中最大生命周期

等待2MSL的根本原因，为了实现TCP全双工可靠的释放

+ 在第4次挥手时，C端向S端发送最后的ACK，也可能会丢失，导致S端收不到该确认报文。当S端在1MSL时间内没有接收到C端发送的ACK确认报文时，S端会再次向C端**重新发送FIN报文（重传机制）**。C端收到FIN报文后，会重新发送最后一个ACK报文，直到S端能成功的接收到ACK报文，连接才真正的断开。

为了使就得数据包在网络中因过期而消失（防止被动方收到主动方发送的过期数据）

+ 先假设没有TIME_WAIT状态的限制，如果当前有一个TCP连接（local_ip，local_port，remote_ip，remote_port）在断开的同时，以相同的4元组去建立新的连接。那么TCP协议栈就无法区分前后两条tcp连接是不同的，在它看来就一条连接。导致前一条TCP已经关闭的连接发送出去的数据，，通过之后的新连接，仍然可以发送给s端

### 自旋锁和互斥锁有什么区别

自旋锁就是一直死等，用cas乐观锁判断是否加锁成功，不成功则一直轮询

互斥锁就是如果加锁失败的话，线程就会进入休眠等待状态，等待锁释放后被内核唤醒

### 分别什么场景下用什么锁

这个要根据业务的情况决定

如果你判断锁等待时间不会过长，那么就用自旋锁。因为线程上下文的切换会比cpu空转更耗时

如果你判断锁等待时间很长，那么这个时候就应该用互斥锁，让线程进入休眠。不应该空转cpu浪费资源

### golang的多态怎么实现

golang多态的实现是通过interface接口类实现。interface接口类定义要实现的方法规范。而只要遵守这个方法规范的结构体，都能被这个interface接口类所赋值

### 对golang的内存管理有了解吗

Golang的内存管理模型是在TCMalloc库上做的修改，大体上都相同，这里介绍的是**TCMalloc**

Golang内存管理分为3级缓存结构，每一层都是使用了内存池的模式进行统一申请和释放。

+ 第一层缓存（MCache）：MCache缓存是每个线程独享的。一个MCache内部由多个链表组成，每个链表（Size Class）要申请的内存都不一样，例如（8b链表，16b链表等等），当链表中没有空闲的内存时则会向下（MCentral）申请内存。
+ 第二层缓存（MCentral）：MCentral为中央缓存，是所有线程共享，所以与CentralCache获取内存交互是需要加锁的。内部实现也跟MCache一样，为多个链表组成。当MCache申请不到内存时，会从MCentral申请。MCentral会到对应的链表（Size Class）中申请，如果还有空闲则返回，如果没有则再向下（MHeap）申请。
+ 第三层缓存（MHeap）：前两层缓存主要解决小对象申请问题。如果是中大对象申请则直接到MHeap申请内存。MHeap内部实现分为两个方式链表和集合管理。第一个方式也是由多个链表组成，每个链表表示要申请的大小为1Page（8K）倍数（1Page，2Page等等），128Page以内的中对象申请都由链表方式申请。第二个方式是由集合申请大对象。当第三层缓存申请不到内存时，则会向操作系统堆区申请内存。

### gmalloc你有了解吗

没有了解

### 缓存改造的项目介绍下

+ 背景：索引服务是提供查询正排数据的服务，该服务需要将数据缓存到本地进而提升服务性能。
+ 工作职责：旧的缓存方案是 groupCache 的 LRU 算法。该算法锁粒度过大，会造成 CPU 竞争降低并发能力。GC 会触发 STW，频繁 GC 严重影响性能。算法没有对内存做好限制，是根据 item数量进行保存，当存在大量 item 的 value 过大时，会撑爆内存，导致服务异常。解决方案：使用了 BigCache 第三方库，该库使用了分片机制和 map 不存指针的方式提升性。
+ 项目结果：cpu 下降了 40%，内存下降 50%，接口 p99 从 20ms 下降到了 5ms

### redis的读压力也会扛不住吗

因为防止异常情况redis崩溃，导致redis服务不可用，所以我们才会做一个二级缓存。而且二级缓存的性能会比读redis高不少

### redis的请求量有多少

ops高峰值会有200W