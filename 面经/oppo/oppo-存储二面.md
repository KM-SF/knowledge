### redis有几种部署模式

主从复制，哨兵模式，cluster模式，sharding模式

### redis主从复制的原理

redis的数据一致性，他们主从之间数据同步有：增量复制和全量复制

> 全量复制：
>
> + 主节点通过bgsave命令fork子进程进行RDB持久化，RDB持久化过程是非常消耗CPU，内存（页表复制），硬盘IO
> + 主节点通过网络将RDB文件发送给从节点，对主从节点的带宽都会带来非常大的消耗
> + 从节点清空老数据，载入新RDB文件（载入过程是阻塞的，无法响应客户端命令）。
> + 如果从节点执行了bgrewriteaof，也会带来额外的消耗（aof复制也需要消耗资源）

> 增量复制：
>
> + 复制偏移量：执行复制的双方，主从节点分别会维护一个复制偏移量offset
> + 复制积压缓存区：主节点内部维护一个固定长度的，先进先出队列作为复制积压缓冲区，**当主从节点offset的差距超过换缓存区长度时，将无法进行增量复制，只能执行全量复制**

### redis的cluster集群分片是怎么做的

+ 通过哈希的方式，将数据分片，每个节点均分存储一定的哈希槽（哈希值）区间的数据，默认分配了16384个槽位
+ 数据分片存储在多个互为主从的多个节点上（16384个槽均分到每个节点上）
+ 数据先写入主节点，再同步到从节点（支持配置为阻塞同步）
+ 同一分片多个节点间的数据不保证强一致性
+ 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点
+ 扩容时需要把旧节点的数据迁移一部分到新节点
+ 在redis cluster架构下，每个redis要放开两个端口，例如：一个6379，另外一个就是加上1W的端口16379。另外一个端口是用来进行节点间通信的，也就是cluster bus的通信。用来进行故障检测，配置更新，故障转移授权。
+ cluster bus用了另外一种二进制的协议，gossip协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间

### cluster集群扩容的时候，key和节点的映射关系怎么处理

扩容原理： 

1. redis cluster可以实现对节点的灵活上下线控制
2. 3个主节点分别维护自己负责的槽和对应的数据，如果希望加入一个节点实现扩容，就需要把一部分槽和数据迁移和新节点
3. 每个master把一部分槽和数据迁移到新的节点node04

key和hash槽之间的映射关系不变，hash槽和节点之间的映射关系会改变。导致key和节点之间的映射关系发生变化。

### 让你自己设计集群扩容的话，这个映射关系要怎么做

因为普通的hash映射是直接取模，那么会扩容和缩容的情况会导致所有节点的映射关系失效，那这个时候可以采取一致性hash算法

### 普通的一致性hash算法有什么问题，会有什么变种吗

如果机器较少的情况，容易出现节点的hash环分部不均衡的情况，导致各个节点映射的数据数量严重不均衡。

如果机器较多时，数据在hash环上就分布的均匀。

但是一般部署的物理节点有限，所以我们需要使用模拟的方式，模拟出多个虚拟节点。

### redis有遇到大key的问题嘛

没有遇到

### 如果有大key的情况，对redis有什么影响吗

因为redis他的网络io处理是单线程的，所以如果是一个大key的情况，会导致读取数据比较慢，导致线程阻塞，后面的请求都无法及时响应处理

### 介绍你一个项目

#### 1. 项目背景：

日常中我们使用的硬盘大多数都是本地硬盘，直接插到主机上面进行本地读写。
而我们分布式存储提供了远端硬盘的模式，可以通过网络进行远端IO读写，简单理解就是网盘。
而使用的网络通信协议是iscsi，我们基于iscsi实现了服务端，客户端由其他厂商实现。

#### 2. 工作内容：

分布式存储就需要面临解决三高问题：高性能（高IO吞吐量），高可用（保证业务不中断），高可靠（数据一致性）。

##### 2.1 第一个问题：高性能

我们使用了开源框架tgtd作为服务端，而它是单线程模型，当 CPU 跑满时性能指数不达标，无法实现高性能，所以我们要对其性能优化。
原生tgtd是单反应堆模型，现在改造成多反应堆提升网络通信性能。
也没有线程池，所有处理都在主线程处理。所以引进了IO线程池和共享请求队列，对IO进行异步处理。
而由单线程改成多线程后，势必会遇到并发控制问题。
我们使用了互斥锁，读写锁，条件变量解决问题。在使用锁过程中还遇到不可重入锁的问题。
对tgtd改进完的成果是：读性能提升40%，写性能提升10%

##### 2.2 第二个问题：高可用

tgtd是基于TCP/IP进行网络传输的。我们一个物理网口坏了或者主机宕机了引起物理IP不可用，导致业务中断。所以我们需要一个保活机制，保证IP一直可用。
我们引入了Keepalived组件，该组件提供了虚拟IP的机制，为集群每台主机配置虚拟IP。
当某台主机不可用时，则该主机上的虚拟IP会被其他节点接管。从而保证IP一直可用

##### 2.3 第三个问题：高可靠

高可靠 是由我们的后台存储模块负责的。tgtd只是将io请求转发给后端存储模块，不负责副本一致性问题。

##### 2.4 最后说下：负载均衡

项目初期并没有负载均衡这个功能，每个主机接入的流量不均衡，导致数据倾斜。
为了充分利用好每个主机资源，使流量尽可能均摊到每个主机，才实现了负载均衡功能。
负载均衡算法中有轮询，随机，加权轮询，平滑加权轮询算法，最少活跃数等。
基于服务情况，最后选择了最少活跃数算法。重定向到硬盘访问个数最少的主机。
（情况：我们这个是保持长连接进行请求处理。当某些主机挂了，客户端重新发起连接，如果使用轮询算法的会还是会不均衡）

### 你们整个项目改造的后的吞吐量级

改造后吞吐量可以达到几十万

### epoll的底层实现原理

+ 原理：
  + epoll_create：创建一个epoll对象。一个epoll对象主要是：一个红黑树和一个就绪队列
  + epoll_ctl：将监听的fd挂到红黑树上，并且跟网卡驱动设置回调函数。回调函数的作用是：当网卡驱动监听到这个fd有事件发生时，将这个fd拷贝到就绪队列中
  + epoll_wait：数据准备阶段已经完成。将就绪队列中的数据从内核空间拷贝到用户空间
+ 底层实现原理：

  + 当内核初始化epoll时，会开辟一块内核高速cache区，用于安置我们监听的socket，这些socket会以红黑树的形式保存在内核的cache里，以支持快速的查找，插入，删除．同时，建立了一个list链表，用于存储准备就绪的事件．所以调用epoll_wait时，在timeout时间内，只是简单的观察这个list链表是否有数据，如果没有，则睡眠至超时时间到返回；如果有数据，则在超时时间到，拷贝至用户态events数组中．
  + 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。

### 红黑树上面的保存的是什么信息

保存的是监听的fd

### 红黑树上的树根保存的是什么

### 红黑树有什么特点

+  每个节点非红即黑
+  根节点是黑的;
+  每个叶节点（叶节点即树尾端NULL指针或NULL节点）都是黑的;
+  **如果一个节点是红色的，则它的子节点必须是黑色的。**
+  **对于任意节点而言，其到叶子点树NULL指针的每条路径都包含相同数目的黑节点;**

### 访问时间复杂度是多少

红黑树在查找，插入删除的性能都是O(logn)，且性能稳定

### 描述下进程A发送数据到进程B，整个过程中的数据流

1. read 调用导致**用户态到内核态的一次变化**，同时，第一次复制开始：DMA（Direct Memory Access，直接内存存取，即不使用 CPU 拷贝数据到内存，而是 DMA 引擎传输数据到内存，用于解放 CPU） 引擎**从磁盘读取文件，并将数据放入到内核缓冲区。**
2. 发生第二次数据拷贝，即：**将内核缓冲区的数据拷贝到用户缓冲区**，同时，**发生了一次用内核态到用户态的上下文切换。**
3. 发生第三次数据拷贝，我们调用 write 方法，**系统将用户缓冲区的数据拷贝到 Socket 缓冲区**。此时，**又发生了一次用户态到内核态的上下文切换。**
4. 第四次拷贝，**数据异步的从 Socket 缓冲区，使用 DMA 引擎拷贝到网络协议引擎**。这一段，不需要进行上下文切换。
5. write 方法返回，**再次从内核态切换到用户态。**

### 一次收发数据有几次的用户态和内核态的切换

**传统IO需要发生4次数据拷贝，4次上下文切换**

### 有没有机制可以减少数据的拷贝和上下文切换

零拷贝机制

第一次使用 DMA 引擎从文件拷贝到内核缓冲区

第二次从内核缓冲区将数据拷贝到网络协议栈；内核缓存区只会拷贝一些 offset 和 length 信息到 SocketBuffer，基本无消耗。

**总结：sendfile正常情况下需要2次数据拷贝，2次上下文切换**

### 网络除了零拷贝机制还有什么优化手段吗

PageCache：磁盘高速缓存，**来缓存最近被访问的数据**，当空间不足时淘汰最久未被访问的缓存。

### pagecache的脏页怎么处理，怎么落盘的

https://zhuanlan.zhihu.com/p/436313908

Linux内核由于存在page cache, 一般修改的文件数据并不会马上同步到磁盘，会缓存在内存的page cache中，我们把这种和磁盘数据不一致的页称为脏页，脏页会在合适的时机同步到磁盘。为了回写page cache中的脏页，需要标记页为**脏（dirty）**。

当前 Linux 下以两种方式实现文件一致性：

1. **Write Through（写穿）**：向用户层提供特定接口，应用程序可主动调用接口来保证文件一致性；
2. **Write back（写回）**：系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块，这是默认的 Linux 一致性方案；

### 脏页刷盘是调用fflush还是fsync？这两个有什么区别

sync: 把page cache中的高速缓存的所有文件的脏页，super block, 以及inode本身刷新到磁盘。

fsync: 把制定文件的脏页写到磁盘，包括page cache和inode本身。

fdatasync: 只把制定文件的page cache写入磁盘，忽略inode本身。

fflush: 强制将该文件对应的用户空间glibc里面的的4KB的buffer写入内核缓存。

### pagecache脏页刷到磁盘后，这个时候刚好掉电了，你认为这个数据是否会丢失

https://www.modb.pro/db/228651

一次写数据的典型流程（不考虑异常和其它特殊情况）：

1. 数据在用户态的 buffer 中，调用 write 将数据传给内核；

2. 数据在 Page Cache 中，返回写入的字节数（成功返回）；

3. 内核将数据刷新到磁盘缓存区。
4. 磁盘缓冲区写到磁盘

第二步如果返回成功，说明数据已经到达操作系统的Page Cache，可以保证的是如果进程挂了，但是操作系统没挂，数据不会丢失。

如果调用 `fsync` 将数据刷新到磁盘上，返回成功，说明数据已经刷新到硬件上了——我们一般认为如果 `fsync` 返回成功，则表示数据持久化成功。

只有当数据被写入到磁盘缓存或者磁盘介质中之后，才能够保证当系统崩溃之后，数据不会丢失（**如果数据在磁盘缓存中，则需要磁盘具有备份电源**）。

那么需要将内核中的Page Cache中的数据写入到磁盘（缓存）中，我们只需要调用fsync（fdatasync）即可。此时就算机器宕机了，咱们的数据还是安全的。这也就是很多WAL都是fsync刷盘的原因。

数据就已经进入磁盘了。但是却并不能保证数据百分之百的落盘成功。有可能数据在磁盘的缓存中。**此时如果机器掉电了，那我们的数据也就可能会丢失**。针对该问题，目前主要有两种解决办法：备用电源和开启OS的Write Barriers。

### kafka和RocketMQ有什么区别

1. kafka不支持缩容，RocketMQ支持
2. kafka一个消费者可以消费多个主题，RocketMQ只能消费一个主题
3. RocketMQ支持延时队列，死信队列，事务消息

### 他们分别适合用于什么场景

#### 1）kafka

优点：吞吐量非常大，性能非常好，集群高可用

缺点：会丢数据，功能比较单一

使用场景：日志分析，大数据菜鸡

#### 2）RocketMQ

优点：高吞吐，高性能，高可靠，功能非常全面

缺点：开源版功能不如商业版。官方文档和周边生态不够成熟。客户端只支持java

使用场景：几乎是全场景

### 事务消息怎么来实现的

1. 生产者向MQ服务器发送half消息。
2. half消息发送成功后，MQ服务器返回确认消息给生产者。
3. 生产者开始执行本地事务。
4. 根据本地事务执行的结果（`UNKNOW`. `commit`. `rollback`）向MQ Server发送提交或回滚消息。
5. 如果错过了（可能因为网络异常. 生产者突然宕机等导致的异常情况）提交/回滚消息，则MQ服务器将向同一组中的每个生产者发送回查消息以获取事务状态。
6. 回查生产者本地事物状态。
7. 生产者根据本地事务状态发送提交/回滚消息。
8. MQ服务器将丢弃回滚的消息，但已提交（进行过二次确认的half消息）的消息将投递给消费者进行消费。

`half Message`：预处理消息，当broker收到此类消息后，会存储到`RMQ_SYS_TRANS_HALF_TOPIC`的消息消费队列中

`检查事务状态`：Broker会开启一个定时任务，消费`RMQ_SYS_TRANS_HALF_TOPIC`队列中的消息，每次执行任务会向消息发送者确认事务执行状态（提交、回滚、未知），如果是未知，Broker会定时去回调在重新检查。

超时：如果超过回查次数，默认回滚消息。

也就是他并未真正进入Topic的queue，而是用了临时queue来放所谓的`half message`，等提交事务后才会真正的将half message转移到topic下的queue。

### base理论是什么

BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网分布式系统实践的总结，是基于CAP定律逐步演化而来。**其核心思想是即使无法做到强一致性**，但每个应用都可以根据自身业务特点，才用适当的方式来使系统打到**最终一致性**。

+ 基本可用（Basically Available）
+ 软状态（Soft State）
+ 最终一致性（Eventually Consistent）

### cap的理论，业界有哪些产品分别满足什么

**AP模式**

- **eureka服务注册与发现中心集群**
- **mysql数据集群与redis集群**，由于mysql和redis的数据复制都是采用的异步复制，所以mysql数据集群与redis集群都属于AP类型，在集群中获取数据时，会存在数据不一致的情况。

**CP模式**

+ **zoomkeeper服务注册与发现中心集**
+ **Kafka集群（ack=all的配置时）**

### zk什么场景下会丢失可用性

当存在过半节点以上不可用的时候，zk就只能读不能写

### 满足ap的产品有什么

Eureka

在服务B向Eureka2注册成功后，此时，Eureka2还没向Eureka3复制成功就挂掉了，此时，在Eureka的服务注册与发现中心集群中造成了数据不一致。当服务A通过服务注册于发现中心集群通过Eureka3来拿服务B的地址时，就无法拿到。

### 介绍下raft协议

raft共识算法就是三个主要模块：leader选举，数据同步和分区共识

> leader选举
>
> + 所有节点都有一个定时器（每个节点的定时器都是随机值）。该定时器表示收到Leader的心跳包。
> + 如果在定时器内没有收到Leader的心跳包，则自己变成Candidate状态，参与竞选Leader。（选举超时，自己当成候选者）
> + 然后所有任参与选票，候选者的投票只会投给自己，其他人则投给某个候选者（可能出现多个候选者情况）
> + 如果自己的票数过半，则成为新的Leader。
> + 如果此时有人跟自己一样同时竞选Leader，那么这两个人都会在起一个定时器（随机值），进行在一轮的选票

> 数据同步
>
> + 当leader接收到请求后，将该请求写到自己的日志中
> + leader将该请求发送给其他节点，要求其他节点也写到日志中
> + 其他节点将请求写到日志后，回一个ack给leader
> + leader统计有过半节点回复了ack后，将请求数据更新
> + leader发送请求给其他节点，要求其他节点将数据更新

> 分区共识
>
> + 集群5个节点，一开始1个leader和4个Follower
> + 然后网络出现问题，把1个leader和1个Follower划分为一个区A。而其他三个节点为另外一个区B
> + 另外三个节点为一个区A后，进行重新选举leader
> + 有个clientA向分区A发送请求，但是这个时候分区只有两个节点，没有超过半数所以该请求无效
> + 有个clientB向分区B发送器请求，有过半节点同意该请求，所以请求有效，更新数据
> + 当网络恢复后，出现了两个leaderA和leaderB。因为leaderB是新一代的leader，所以leaderA放弃做leader。并且将分区B的数据更新到分区A的每个节点上
> + 问题：一致性并不一定代表完全正确性！三个可能结果：成功，失败，unknown

### leader选举过程中，A投票给B，那需要满足什么条件。投票过程有什么限制（我回答了zab协议的情况）

raft选举的三大规约：https://www.zhihu.com/zvideo/1417543566558556161

1. 一次选举过程中，必须超过半数投票
2. 节点只能响应任期号（term）大于或者等于自己任期的请求
3. 同一个任期内（term）只能选举一次。（follower只能投票给一个人）

### 数据同步如果follower数据落后比较多的话，有什么机制可以快速同步吗

follower节点有个数据的index，leader也有一个数据的index。快速同步机制就是：做增量同步，只负责同步这部分数据

### 如果有个节点故障了很久，回复后那还是用增量同步吗？

不清楚。估计跟redis的一样。应该是采用全量同步机制：（**猜测**）

1. 故障节点的term编号跟leader的不一样，采用全量同步机制
2. 故障节点和leader之间的偏移量offset超过了缓存区大小，采用全量同步机制

### cpp的多态怎么实现的

多态的实现主要分为**静态多态和动态多态**

+ 静态多态主要是通过**重载和模板技术**实现，在编译的时候确定
+ 动态多态是用虚函数机制实现的**（覆盖）**，在运行期间动态绑定。举个例子：一个父类类型的指针指向一个子类对象时候，使用父类的指针去调用子类中重写了的父类中的虚函数的时候，会调用子类重写过后的函数，在父类中声明为加了virtual关键字的函数，在子类中重写时候不需要加virtual也是虚函数。

### 静态多态和动态多态是在哪个阶段确定的

静态多态：编译时确定

动态多态：运行时动态绑定

### 静态多态能在编译阶段就确定的原因是什么

静态多态在编译阶段，会将函数转化成符号的形式（函数名，参数1，参数2。。。）加入到符号表中。所以在编译阶段实际上是不同的函数。

### 动态多态的实现原理

动态多态是用虚函数机制实现的**（覆盖）**。虚函数的实现：**在有虚函数的类中，类的最开始部分是一个虚函数表的指针，这个指针指向一个虚函数表，表中放了虚函数的地址，实际的虚函数在代码段(.text)中。当子类继承了父类的时候也会继承其虚函数表，当子类重写父类中虚函数时候，会将其继承到的虚函数表中的地址替换为重新写的函数地址。使用了虚函数，会增加访问内存开销，降低效率。**

### 虚函数表是每个类都有一个，还是父子类同享一个

一个类有一个虚表，在多继承情况下，有多少个基类就有多少个虚函数表，每个对象就有多少个虚函数指针表。前提是基类要有虚函数才算上这个基类。
同一个类的不同实例对象，公用一份虚函数表！它们的 Vptr 指向相同的虚函数表。

### 虚函数表保存的是什么

保存的是虚函数的地址

### 一个空类（没有变量和函数），那sizeof大小是多少

sizeof的结果是：**1**

### 为什么是1呢？同一个空类我创建10w个对象，那大小都是1吗

原因：空类型的实例中不包含任何信息，本来求sizeof应该是0，但是当我们声明该类型的实例的时候，它必须在[内存](https://so.csdn.net/so/search?q=内存&spm=1001.2101.3001.7020)中占有一定的空间，否则无法使用这个实例。由编译器决定占用多少内存。在Visual Studio 中。每个空类型的实例占用一个字节的空间。

### 互斥锁和自旋锁有什么区别

自旋锁就是一直死等，用cas乐观锁判断是否加锁成功，不成功则一直轮询

互斥锁就是如果加锁失败的话，线程就会进入休眠等待状态，等待锁释放后被内核唤醒

### 互斥锁和自旋锁的应用场景是什么

这个要根据业务的情况决定

如果你判断锁等待时间不会过长，那么就用自旋锁。因为线程上下文的切换会比cpu空转更耗时

如果你判断锁等待时间很长，那么这个时候就应该用互斥锁，让线程进入休眠。不应该空转cpu浪费资源

+ 编程题1：单链表反转
+ 编程题2：二叉树的层次遍历，偶数层从左到右输出，奇数层从右到左输出
+ 你看新的工作机会考虑哪一点
+ 你觉得接下来要从事哪个方向

