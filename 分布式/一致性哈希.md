# 一. 介绍

分布式系统中对象（数据）与节点（服务器）的映射关系，传统方案是使用对象的哈希值，对节点个数取模，再映射到相应编号的节点

这种方案在节点个数变动时，绝大多数对象的映射关系会失效而需要迁移，因为取模的节点数发生变化了，需要将原本节点的数据重新取模，重新存放；

而一致性哈希算法中，当节点个数变动时，映射关系失效的对象非常少，迁移成本也非常小

# 二. 分布式取模算法

## 1. 实现原理

1. 无论增删改查，都是将数据通过hash函数计算出一个key
2. 将这个key和目前有N台机器进行取模运算，得到对应的机器编号。
3. 就知道这个数据的操作都是这个机器上

## 2. 问题

当增加或删除机器(N变化)时，代价会非常高，所有的数据都不得不根据ID重新计算一遍哈希值，并将哈希值对新的机器数N进行mod操作，之后还要进行大规模的数据迁移。

# 三. 一致性hash算法

## 1. 实现原理

### 1.1 映射方案

![](/分布式/images/一致性hash算法.jpg)

#### 1.1.1 生成hash环

使用一个hash函数，得到一个hash环范围 [0, 2^32)

将各个hash值再hash环上进行分部部署：时钟12点位置为0，按顺时针方向递增，临近12点的左侧位置为2^32-1。

#### 1.1.2 生产hash节点

机器的id或者ip通过相同的hash函数得到对应的hash节点，就知道该机器在hash环上的位置。

例如上图的Node A/B/C/D

#### 1.1.3 数据映射到环中

数据也通过相同的hash函数得到hash值，然后映射到上面的hash环中

例如上面的Object A/B/C/D

#### 1.1.4 数据映射到节点

在数据和节点都映射到hash环上后，就可以确定这个数据是属于哪个节点了

我们只需要从数据的hashhuan上位置，沿着hash环顺时针方向查找第一个节点就是

例如：ObjectA->NodeA, ObjectB->NodeB, ObjectC->NodeC, ObjectD->NodeD

### 1.2 增加节点

![](/分布式/images/一致性hash增加节点.jpg)

#### 1.2.1 现实场景

服务器扩容时增加节点。比如要在 Node B/C 之间增加节点 Node X。

#### 1.2.2 重新映射

新增节点和上一个节点之间的hash环上数据需要重新映射给新节点，并且将这些数据从旧节点中删除。

例如：NodeB~NodeX之间的hash环上数据都映射给NodeX，而NodeC需要将这些数据删除。

只会影响新增节点和上一个节点之间的数据。例如：NodeB和NodeX之间的数据，因为按照1.1.4的的规则，这些数据要给NodeX，从NodeC删除。其他位置的数据无需做调整

### 1.3 删除节点

![](/分布式/images/一致性hash删除节点.jpg)

#### 1.3.1 现实场景

服务器缩容时删除节点，或者有节点宕机。如下图，要删除节点 Node C：

#### 1.3.2 重新映射

删除节点和上一个节点之间的hash环上数据需要重新映射给删除节点的下一位

例如：NodeB~NodeC之间的hash环上数据都映射给NodeD

只会影响删除节点和上一个节点之间的数据。例如：NodeB和NodeC之间的数据，因为按照1.1.4的的规则，这些数据要给NodeD。其他位置的数据无需做调整

## 2. 虚拟节点

### 2.1 问题场景

如果机器较少的情况，容易出现节点的hash环分部不均衡的情况，导致各个节点映射的数据数量严重不均衡。

如果机器较多时，数据在hash环上就分布的均匀。

但是一般部署的物理节点有限，所以我们需要使用模拟的方式，模拟出多个虚拟节点。

### 2.2 算法思路

![](/分布式/images/一致性hash虚拟节点.jpg)

#### 2.2.1 虚拟节点

对物理主机进行模拟虚拟节点。例如虚拟节点NodeA1,NodeA2,NodeA3都同属于NodeA，实际上所有数据都是落到NodeA。

然后对虚拟节点NodeA1进行hash函数计算得到hash值，再映射到hash环上

#### 2.2.2 数据映射

数据映射到hash环上后就知道是哪个虚拟节点

然后再通过查询虚拟节点属于哪个物理节点，从而指向真实的物理节点。

# 四. 参考

https://blog.csdn.net/kefengwang/article/details/81628977