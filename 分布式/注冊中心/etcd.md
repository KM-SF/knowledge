# 一. etcd

## 1. 介绍

一个用于配置共享和服务发现的键值存储系统，**并通过 Raft 一致性算法处理和确保分布式一致性**，解决了分布式系统中数据一致性的问题。

etcd 设计为提供高可用、强一致性的**小型 kv 数据存储服务**

而且作为一款分布式、可靠的键值存储组件，**etcd 常用于微服务架构中的服务注册与发现中心**，相较于 ZooKeeper 部署更简单，而且具有数据持久化、支持 SSL 客户端安全认证的独特优势。

etcd 可集中管理配置信息。服务端将配置信息存储于 etcd，客户端通过 etcd 得到服务配置信息，etcd 监听配置信息的改变，发现改变通知客户端。

而 etcd 满足 CAP 理论中的 **CP（一致性和分区容错性）** 指标，由此我们知道，etcd 解决了分布式系统中一致性存储的问题。

## 2. 特点

etcd 可以用来构建高可用的分布式键值数据库，总结来说有如下特点。

+ 简单：安装简单，且为用户提供了 HTTP API，使用起来也很简单；
+ 存储：数据分层存储在文件目录中，类似于我们日常使用的文件系统；
+ Watch 机制：Watch 指定的键、前缀目录的更改，并对更改时间进行通知；
+ 安全通信：支持 SSL 证书验证；
+ 高性能：etcd 单实例可以支持 2K/s 读操作，官方也有提供基准测试脚本；
+ 一致可靠：基于 Raft 共识算法，实现分布式系统内部数据存储、服务调用的一致性和高可用性；

在分布式系统或者 Kubernetes 集群中，etcd 可以作为服务注册与发现和键值对存储组件。 

## v2 和 v3 比较

+ 使用 gRPC + protobuf 取代 http + json 通信，提高通信效率；gRPC 只需要一条连接；http是每个请求建立一条连接；protobuf 加解密比json加解密速度得到数量级的提升；包体也更小；
+ v3 使用 lease （租约）替换key ttl自动过期机制；
+ v3 支持事务和多版本并发控制（一致性非锁定读）的磁盘数据库；而 v2 是简单的kv内存数据库；
+ v3 是扁平的kv结构；v2是类型文件系统的存储结构；

# 二. API

#### 1. 设置

PUT key val

#### 2. 删除

DEL key

#### 3. 获取

GET key
GET keyfrom keyend
GET --prefix key

#### 4. 监听

用来实现监听和推送服务

WATCH key
WATCH --prefix key

#### 5. 事务

用于分布式锁以及leader选举；保证多个操作的原子性；确保多个节点数据读写的一致性

TXN if/ then/ else ops

####  6. 租约

用于集群监控以及服务注册发现

创建一个租约：lease grant       

续约：lease keep-alive

枚举所有的租约：lease list

销毁租约：lease revoke 

获取租约信息：lease timetolive

####    7. 锁

LOCK lockname

# 三. 应用

`etcd` 在稳定性、可靠性和可伸缩性上表现极佳，同时也为云原生应用系统提供了协调机制。**`etcd` 经常用于服务注册与发现的场景，此外还有键值对存储、消息发布与订阅、分布式锁等场景。**

## 1. 键值对存储

`etcd` 是一个用于键值存储的组件，存储是 `etcd` 最基本的功能，其他应用场景都建立在 `etcd` 的可靠存储上。比如 `Kubernetes` 将一些元数据存储在 `etcd` 中，将存储状态数据的复杂工作交给 `etcd`，`Kubernetes` 自身的功能和架构就能更加稳定。

## 2. 服务注册与发现

`etcd` 基于 `Raft` 算法，能够有力地保证分布式场景中的一致性。各个服务启动时注册到 `etcd` 上，同时为这些服务配置键的 `TTL` 时间。注册到 `etcd` 上面的各个服务实例通过心跳的方式定期续租，实现服务实例的状态监控。

![](/分布式/注冊中心/images/服务发现.jpg)

## 3. 消息发布与订阅

通过构建 `etcd` 消息中间件，服务提供者发布对应主题的消息，消费者则订阅他们关心的主题，一旦对应的主题有消息发布，就会产生订阅事件，消息中间件就会通知该主题所有的订阅者。

![](/分布式/注冊中心/images/发布订阅.png)

## 4. 分布式锁

分布式系统中涉及多个服务实例，存在跨进程之间资源调用，对于资源的协调分配，单体架构中的锁已经无法满足需要，需要引入分布式锁的概念。`etcd` 基于 `Raft` 算法，实现分布式集群的一致性，存储到 `etcd` 集群中的值必然是全局一致的，因此基于 `etcd` 很容易实现分布式锁。

## 5. 负载均衡

将服务器注册到etcd中。etcd定期检查服务器的监控状态。当客户端向etcd查询健康的服务器，etcd通过负载均衡算法，返回一个较好的服务器。客户端再跟服务器建立链接

![](/分布式/注冊中心/images/负载均衡.jpg)

# 四. 原理

## 1. 架构

![](/分布式/注冊中心/images/架构图.jpg)

+ **boltdb**：是一个单机的支持事务的 kv 存储，etcd 的事务是基于 boltdb 的事务实现的；boltdb 为每一个 key 都创建一个索引（B+树）；该 B+ 树存储了 key 所对应的版本数据；
+ **wal（write ahead log）**：预写式日志实现事务日志的标准方法；执行写操作前先写日志，跟 mysql中 redo 类似，wal 实现的是顺序写，而若按照 B+ 树写，则涉及到多次 io 以及 随机写；
+ **snapshot**： 快照数据，用于其他节点同步主节点数据从而达到一致性地状态；类似 redis 中主从复制中 rdb 数据恢复；流程：1. leader生成 snapshot；2. leader向follower发送snapshot；3. follower接收并应用snapshot；
+ gRPC Server：客户端和服务器通信的机制。也是etcd 与其他 etcd节点之间的通信和信息同步；

## 2. 数据版本号控制

+ term：leader任期，leader切换时term加一；全局单调递增，64bits；
+ revision：etcd 键空间版本号，key发生变更，则revision加一；全局单调递增，64bits；用来支持MVCC；
+ create_revision：创建数据时，对应的版本号；
+ mod_revision：数据修改时，对应的版本号；
+ version：当前的版本号；标识该val被修改了多少次；

## 3. 存储原理

![](/分布式/注冊中心/images/存储机制.jpg)

etcd 为每个key创建一个索引，一个索引对应着一个B+树（磁盘）；

B+树key为 revision，B+节点存储的值为value；B+ 树存储着 key 的版本信息从而实现了 etcd 的 mvcc；

为了加速索引数据，在内存中维持着一个 B 树。B 树key为 key，value 为该 key 的 revision

etcd 不会任由版本信息膨胀，通过定期的 compaction 来清理历史数据；

## 4. 读写机制

etcd 是**串行写，并发读**

并发读写时（读写同时进行），读操作是通过 B+ 树 mmap 访问磁盘数据；写操作走日志复制流程；可以得知如果此时读操作走 B 树出现脏读幻读问题；通过 B+ 树访问磁盘数据其实访问的事务开始前的数据，由 mysql 可重复读隔离级别下 MVCC 读取规则可知能避免脏读和幻读问题；
并发读时，可走内存 B 树；

------

参考：

https://blog.csdn.net/wohu1104/article/details/115764681