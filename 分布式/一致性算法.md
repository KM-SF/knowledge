# CAP原则

+ CAP定理又称CAP原则，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），最多只能同时三个特性中的两个，三者不可兼得。
+ Consistency (一致性)：更新操作成功并返回客户端后，所有节点在同一时间的数据完全一致，这就是分布式的一致性。一致性的问题在并发系统中不可避免，对于客户端来说，一致性指的是并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。
+ Availability (可用性)：服务一直可用，而且是正常响应时间。好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。
+ Partition Tolerance (分区容错性)：分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务。分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，对于用户而言并没有什么体验上的影响。

# 强一致性算法 -- Paxos

## Basic Paxos

+ Client：系统外部角色，请求发起者（像民众）
+ Proposer：接收Client请求，向集群提出提议（propose）。并再冲突发生时，起到冲突调解的作用。（像议员，替民众提出议案）
+ Acceptor（Voter）：提议投票和接收者，只有在形成法定人数（Quorum，一般即为majority多数派）时，提议才会最终被接收（像国会）
+ Learner：提议接受者，backup，备份，对集群一致性没有什么影响（像记录员）

### 步骤

1. Prepare：proposer提出一个提案，编号为N。此时N大于这个proposer之前提出的提案编号。请求acceptors的大多数接受。
2. Promise：如果N大于此acceptor之前接受的任何提案编号则接受，否则拒绝。（一次只能接受一个提案，且以大的为主）
3. Accept：如果达到了多数派，proposer会发出accept请求，此请求包含提案编号N，且提案内容
4. Accepted：如果此acceptor在此期间没有收到任何编号大于N的提案，则接受此天内容，否则忽略

### 问题

+ 活锁：
  + 由于有一个proposerA发起提案1给acceptor。acceptor超过半数同意告诉proposerA处理该提案，让proposerA把内容告诉acceptor。
  + 但是在proposerA告诉acceptor内容的时候，又有一个proposerB发起了提案2给acceptor。然后acceptor超过半数同意，此时的提案2的编号大于提案1，则放弃处理提案1而改成处理提案2。
  + proposerA发现提案1被拒绝了，又发起proposerA发起提案3（内容相同）。然后又出现上面的情况，一直死循环。
  + **解决方案：当proposerB发现提案发生冲突了，然后等待随机时间再提交提案。**
+ 难实现，且效率低（2轮RPC，一轮发起提案，一轮发送提案内容）

## Multi Paxos

+ Leader：唯一的propser，所有请求都需要经过Leader
+ 需要有第一次提案，该提案的作用是选举Leader

# 强一致性算法 -- Raft

+ **动画演示：http://thesecretlivesofdata.com/raft/**
+ 场景演示：https://raft.github.io/
+ 划分三个子问题：
  + 选举Leader
  + 日志同步到其他节点上
  + 所有节点的共识一致（分区共识）
+ 重定义角色：
  + Leader（一个集群只能有一个Leader）
  + Follower
  + Candidate（临时状态，当Leader挂了，需要重新选举Leader的时候。Follower改变成Candidate，然后参加竞选）

+ Leader选举过程：
  + 所有节点都有一个定时器（每个节点的定时器都是随机值）。该定时器表示收到Leader的心跳包。
  + 如果在定时器内没有收到Leader的心跳包，则自己变成Candidate状态，参与竞选Leader
  + 如果自己的票数过半，则成为新的Leader。
  + 如果此时有人跟自己一样同时竞选Leader，那么这两个人都会在起一个定时器（随机值），进行在一轮的选票
+ 日志同步过程：
  + 当leader接收到请求后，将该请求写到自己的日志中
  + leader将该请求发送给其他节点，要求其他节点也写到日志中
  + 其他节点将请求写到日志后，回一个ack给leader
  + leader统计有过半节点回复了ack后，将请求数据更新
  + leader发送请求给其他节点，要求其他节点将数据更新
+ 节点共识：
  + 集群5个节点，一开始1个leader和4个Follower
  + 然后网络出现问题，把1个leader和1个Follower划分为一个区A。而其他三个节点为另外一个区B
  + 另外三个节点为一个区A后，进行重新选举leader
  + 有个clientA向分区A发送请求，但是这个时候分区只有两个节点，没有超过半数所以该请求无效
  + 有个clientB向分区B发送器请求，有过半节点同意该请求，所以请求有效，更新数据
  + 当网络恢复后，出现了两个leaderA和leaderB。因为leaderB是新一代的leader，所以leaderA放弃做leader。并且将分区B的数据更新到分区A的每个节点上

+ 问题：一致性并不一定代表完全正确性！三个可能结果：成功，失败，unknown

# 强一致性算法 -- ZAB

+ 基本与raft相同
+ raft保证日志连续性，心跳方向为leader至Follower。ZAB则相反